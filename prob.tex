\chapter{Элементы теории ошибок}
\label{ch:prob}

% Не претендуя ни в коей мере на полноту или строгость, мы изложим наиболее
% базовые понятия и результаты теории вероятностей, без которых нельзя обойтись
% при описании погрешностей.

% Для работы со случайными величинами необходимо понимать, что они не являются
% полностью произвольными, а подчиняются некоторым законам. Эти законы называются
% распределениями.

Результат любого измерения не определён однозначно и имеет случайную составляющую.
Поэтому адекватным языком для описания погрешностей является язык вероятностей.
Тот факт, что значение некоторой величины \textquote{случайно}, не означает, что
она может принимать совершенно произвольные значения. Ясно, что частоты, с которыми
возникает те или иные значения, различны. Вероятностные законы, которым
подчиняются случайные величины, называют \emph{распределениями}.

\section{Случайная величина}

% 1)Это наглядное определение вероятности иногда называют «частотным» или «фи-
% зическим». Современная аксиоматическая теория вероятностей использует другое, бо-
% лее абстрактное определение, к которому однако нет смысла прибегать в рамках курса
% общей физики. То же касается и вводимых ниже определений среднего, случайной ве-
% личины и т. п. Наша цель — наглядное описание физики явлений, а не построение
% строгой формальной теории.
% 2)Иногда w (ξ) называют просто «функцией распределения». Мы будем избегать этой
% терминологии, поскольку в теории вероятностей «функцией распределения» называют
% другую характеристику случайной величины — вероятность того, что она не превосхо-
% дит некоторое значение. В англоязычной литературе различают соответственно partial
% distribution function (pdf) и cumulative distribution function (cdf).

% Определение понятия вероятности является сложной задачей без однозначно
% правильного решения. В зависимости от области применения, могут применяться
% различные, во многих случаях взаимоисключающие определения. При этом большинство
% определений приводят к одним и тем же практическим выводам, поэтому с прикладной
% точки зрения не так важно, какое из них выбрать.

% \disclaimer{
%     Конкретное определение вероятности не является существенным для практических
% применений, поэтому всем, кому не интересна идейная составляющая вопроса
% рекомендуется пропустить этот раздел и пользоваться "наивным" определением,
% основанном на "здравом смысле".
% }

% Как это ни парадоксально, формальная теория вероятности и математическая
% статистика никак не определяют понятие вероятности. Все теоретические выводы
% основываются на том, что изучаемые величины (вероятности) удовлетворяют
% некоторым формальным правилам (например системе аксиом Колмогорова
% \todo{ссылка}), но никаким образом не определяют как эти величины получить. Со
% времен создания теории вероятности существует два лагеря статистиков, каждый из
% которых использует свое определение. Обсуждение этих формальных моментов лежит
% далеко за пределами данного пособия, так что для упрощения понимания материала а
% также сокращения многих доказательств, в данном пособии мы будем придерживаться
% следующих определений:

Назовём \emph{вероятностью} $P_A$ некоторого события $A$
(например, результата измерения) \emph{долю случаев}, в которых реализуется
данный результат в пределе большого числа измерений $n$:
\[
P_A = \lim_{n\to \infty} \frac{n_A}{n},
\]
где $n_A$ --- количество наблюдений результата $A$.

\emph{Случайной} будем называть величину, значение которой не может быть
\emph{достоверно} определено экспериментатором с абсолютной точностью или
значение которой меняется в каждом последующем опыте неконтролируемым образом.
Согласно такому определению, \emph{любой} результат любых измерений является
случайным, поскольку осведомленность экспериментатора всегда чем-то ограничена.

\note{Хотя понятия вероятности и случайной величины являются основополагающими,
    в литературе нет единства в их определении. Обсуждение формальных тонкостей
    или построение строгой теории лежит за пределами данного пособия.
    Наша цель --- дать упрощенное, но наглядное представление о предмете.
Заинтересованным читателям рекомендуем обратиться к специальной литературе: \cite{idie}.}

% \subsection{Распределения}
% \emph{Распределением случайной величины} $x$ или \emph{функцией плотности
% вероятности} называют такую функцию $f(x)$, что вероятность для нее находиться в
% диапазоне от $x$ до $x+dx$ равна $p(x \in (x_0, x_0+dx)) = f(x_0) dx$. Очевидно
% \todo{не очевидно}, что такое определение годится только для численным величин и
% имеет ряд других ограничений, но в данном пособии мы ограничимся им. Более
% строгие определения можно почерпнуть из специальной литературы (\todo{ссылка}).
% \todo[inline,author=ppv]{Отослать один раз в начале главы к более строгим
% определениям в специальной литературе. Мы заведомо не претендуем на строгость
% или полноту.}

Рассмотрим случайную физическую величину $x$, которая при измерениях может
принимать \emph{непрерывный} набор значений. Пусть
$P_{[x_0,\,x_0+\delta x]}$~--- вероятность того, что результат окажется вблизи
некоторой точки $x_0$ в пределах интервала $\delta x$: $x\in [x_0,\,x_0+\delta x]$.
Устремим интервал
$\delta x$ к нулю. Нетрудно понять, что вероятность попасть в этот интервал
также будет стремиться к нулю. Однако отношение
$w(x_0) = \frac{P_{[x_0,\,x_0+\delta x]}}{\delta x}$ будет оставаться конечным.
Функцию $w(x)$ называют \emph{плотностью распределения вероятности} или кратко
\emph{распределением} непрерывной случайной величины $x$.

% Важно отметить, что в работах по статистике, написанных математиками,
% распределением часто называют не саму функцию $f$, а ее интеграл $F(x_0) =
% \int{f(x)dx}$, который можно определить, не используя бесконечно малые. Такую
% функцию физики называют \emph{интегральным распределением}.

\note{В математической литературе распределением часто называют не функцию
    $w(x)$, а её интеграл $W(x)=\int w(x)\,dx$. Такую функцию в физике принято
    называть \emph{интегральным} или \emph{кумулятивным} распределением. В англоязычной литературе
    для этих функций принято использовать сокращения:
    \emph{pdf} (\emph{probability distribution function}) и
    \emph{cdf} (\emph{cumulative distribution function})
    соответственно.}

\paragraph{Гистограммы.}
Проиллюстрируем наглядно понятие плотности распределение. Результат
большого числа измерений случайной величины удобно представить с помощью
специального типа графика --- \emph{гистограммы}.
Для этого область значений $x$, размещённую на оси абсцисс, разобьём на
равные малые интервалы --- \textquote{корзины>> или <<бины} (\emph{англ.} bins)
некоторого размера $h$. По оси ординат будет откладывать долю измерений $w$,
результаты которых попадают в соответствующую корзину. А именно,
пусть $k$ --- номер корзины; $n_k$ --- число измерений, попавших
в диапазон $x\in [kh,\,(k+1)h]$. Тогда  на графике изобразим \textquote{столбик}
шириной $h$ и высотой $w_{k}=n_{k}/n$.
В результате получим картину, подобную изображённой на рис.~\ref{fig:normhist}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{images/normhist.pdf}
    \caption{Пример гистограммы для нормального распределения ($\limaverage{x}=10$,
$\sigma=1{,}0$, $h=0{,}1$, $n=10^{4}$)}\label{fig:normhist}
\end{figure}

Высоты построенных столбиков будут приближённо соответствовать значению
плотности распределения $w(x)$ вблизи соответствующей точки $x$.
Если устремить число измерений к бесконечности ($n\to \infty$), а ширину корзин
к нулю ($h\to0$), то огибающая гистограммы будет стремиться к некоторой
непрерывной функции $w(x)$.

Самые высокие столбики гистограммы будут группироваться вблизи максимума
функции $w(x)$ --- это \emph{наиболее вероятное} значение случайной величины.
Если отклонения в положительную и отрицательную стороны равновероятны,
то гистограмма будет симметрична --- в таком случае среднее значение $\average{x}$
также будет лежать вблизи этого максимума. Ширина гистограммы будет характеризовать разброс
значений случайной величины --- по порядку величины
она, как правило, близка к среднеквадратичному отклонению $\sigma_x$.

\paragraph{Свойства распределений.}

Из определения функции $w(x)$ следует, что вероятность получить в результате
эксперимента величину $x$ в диапазоне от $a$ до $b$
% или вероятностное содержание интервала $(a,b)$
можно найти, вычислив интеграл:
\begin{equation}
    P_{x\in [a, b]}=\int\limits _{a}^{b}w\!\left(x\right)dx.\label{eq:P}
\end{equation}

Согласно определению вероятности, сумма вероятностей для всех возможных случаев
всегда равна единице. Поэтому интеграл распределения $w(x)$ по всей области
значений $x$ (то есть суммарная площадь под графиком $w(x)$) равен единице:
\[
\int\limits_{-\infty}^{+\infty} w(x)\,dx=1.
\]
Это соотношение называют \emph{условием нормировки}.

\paragraph{Среднее и дисперсия.}

Вычислим среднее по построенной гистограмме. Если размер корзин
$h$ достаточно мал, все измерения в пределах одной корзины можно считать примерно
одинаковыми. Тогда среднее арифметическое всех результатов можно вычислить как
\[
\average{x} \approx \frac{1}{n}\sum_i n_i x_i = \sum_i w_i x_i.
\]
Переходя к пределу, получим следующее определение среднего значения
случайной величины:
\begin{equation}
    \limaverage{x} = \int{x w\,dx},
\end{equation}
где интегрирование ведётся по всей области значений $x$.
В теории вероятностей \limaverage{x} также называют \emph{математическим ожиданием}
распределения.
Величину
\begin{equation}
    \sigma^2 = \limaverage{(x-\limaverage{x})^2}= \int{(x - \limaverage{x})^2 w\,dx}
\end{equation}
называют \emph{дисперсией} распределения. Значение $\sigma$ есть
срекднеквадратичное отклонение в пределе $n\to \infty$. Оно имеет ту
же размерность, что и сама величина $x$ и характеризует разброс распределения.
Именно эту величину, как правило, приводят как характеристику погрешности
измерения $x$.

\paragraph{Доверительный интервал.}
Обозначим как $P_{\left|\Delta x\right|<\delta}$ вероятность
того, что отклонение от среднего $\Delta x=x-\limaverage{x}$ составит величину,
не превосходящую по модулю значение $\delta$:
\begin{equation}\label{eq:confidenceP}
P_{\left|\Delta x\right|<\delta}=\int\limits
_{\limaverage{x}-\delta}^{\limaverage{x}+\delta}w\!\left(x\right)dx.
\end{equation}
Эту величину называют \emph{доверительной вероятностью} для
\emph{доверительного интервала} $\left|x-\limaverage{x}\right|\le\delta$.

% \todo{Что иллюстрирует пример?}
% \example{Пусть есть набор резисторов из одной серии с одной и той же маркировкой,
% соответствующей одному и тому же номиналу их сопротивления. В силу неидеальности
% процесса изготовления, точное значение резистора является случайной величиной,
% средней значение которое должно совпадать с заводским номиналом, но при этом
% будет наблюдаться также некоторый разброс значений. Рассмотрим два измерения: в
% первом будем просто брать все резисторы из одной серии, замерять их
% сопротивления и строить гистограмму результата измерений. В этом случае мы
% увидим картину похожую на рис.~\ref{fig:normhist}.\par
%     Теперь предположим, что перед тем как мы начали проводить свои измерения,
% кто-то взял и отобрал из изучаемой партии все резисторы с сопротивлением
% максимально приближенным к номинальному значению. В этом случае окажется, что в
% нашем измерении вероятность получить сопротивление, близкое к номиналу, будет
% мала. Как следствие, в результатем измерения будет получено \textquote{двух-горбое}
% распределение с провалом посередине. Среднее значение будет таким же, как и в
% первом случае, но разброс будет больше и наиболее вероятное значение (точнее два
% наиболее вероятных значения) не будут совпадать со средним.}


% Предположим, что систематические погрешности малы и займёмся отдельно
% изучением случайных погрешностей. Пусть по результатам многократных
% измерений получен набор значений $\left\{ x_{i}\right\} $, вычислено
% их среднее (\ref{eq:average}) $\average{x}$ и среднеквадратичное
% отклонение (\ref{eq:sigma}) $\sigma_{x}\approx s{}_{x}$. Можно надеяться,
% что измеряемая величина лежит в диапазоне
% \[
% x\in\left(\average{x}-\sigma_{x};\,\average{x}+\sigma_{x}\right).
% \]
% Какова вероятность $P$ того, что результат действительно находится
% в указанном интервале?

% Для ответа на этот вопрос необходимо знать \emph{вероятностный закон},
% которому подчиняется исследуемая величина. Казалось бы, для каждой
% случайной физической величины должен существовать свой особенный закон
% и общую теорию здесь построить невозможно. Это отчасти верно, но оказывается,
% что существует вполне \emph{универсальный} вероятностный закон, называемый
% \emph{нормальным}, которому подчиняются многие случайные величины.
% Рассмотрим его подробнее.

\section{Нормальное распределение}\label{sec:normal}

Одним из наиболее примечательных результатов теории вероятностей является
так называемая \emph{центральная предельная теорема}. Она утверждает,
что сумма большого количества независимых случайных слагаемых, каждое
из которых вносит в эту сумму относительно малый вклад, подчиняется
универсальному закону, не зависимо от того, каким вероятностным законам
подчиняются её составляющие, --- так называемому \emph{нормальному
распределению} (или \emph{распределению Гаусса}).

Доказательство теоремы довольно громоздко и мы его не приводим (его можно найти
в любом учебнике по теории вероятностей). Остановимся
кратко на том, что такое нормальное распределение и его основных свойствах.

Плотность нормального распределения выражается следующей формулой:
\begin{equation}
    \label{eq:normal}
    \boxed{
w_{\mathcal{N}}\!\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\tfrac{(x-\limaverage{x})^
{2}}{2\sigma^{2}}}
    }.
\end{equation}
Здесь $\limaverage{x}$ и $\sigma$
--- параметры нормального распределения: $\limaverage{x}$ равно
среднему значению $x$, a $\sigma$ ---
среднеквадратичному отклонению, вычисленным в пределе $n\to\infty$.

Как видно из рис.~\ref{fig:normhist}, распределение представляет собой
симметричный
\textquote{колокол}, положение вершины которого
соответствует $\bar{x}$ (ввиду симметрии оно же
совпадает с наиболее вероятным значением --- максимумом
функции $w_{\mathcal{N}}(x)$).

При значительном отклонении $x$ от среднего величина
$w_{\mathcal{N}}\!\left(x\right)$
очень быстро убывает. Это означает, что вероятность встретить отклонения,
существенно большие, чем $\sigma$, оказывается \emph{пренебрежимо
мала}. Ширина \textquote{колокола} по порядку величины
равна $\sigma$ --- она характеризует \textquote{разброс}
экспериментальных данных относительно среднего значения.

\note{Точки $x=\bar{x}\pm\sigma$ являются точками
    перегиба графика $w\left(x\right)$ (в них вторая производная по $x$
    обращается в нуль, $w''=0$), а их положение по высоте составляет
    $w\!\left(\bar{x}\pm\sigma\right)/w(\bar{x})=e^{_{-1/2}}\approx0{,}61$
    от высоты вершины.}

Универсальный характер центральной предельной теоремы позволяет широко
применять на практике нормальное (гауссово) распределение для обработки
результатов измерений, поскольку часто случайные погрешности складываются из
множества случайных \emph{независимых} факторов. Заметим, что на практике
для \emph{приближённой оценки} параметров нормального распределения
случайной величины используются \emph{выборочные} значения среднего
и дисперсии: $\limaverage{x}\approx\average{x}$, $s_{x}\approx\sigma_{x}$.

\begin{figure}[h]
    \centering
    \input{images/gauss.pdf_t}
    \caption{Плотность нормального распределения}
\end{figure}

\paragraph{Доверительные вероятности.}
Вычислим некоторые доверительные вероятности \eqref{eq:confidenceP} для нормально
распределённых случайных величин.
\note{Значение интеграла вида $\int e^{-x^{2}/2}dx$
(его называют \emph{интегралом ошибок}) в элементарных функциях не выражается,
но легко находится численно.}
Вероятность того, что результат отдельного измерения $x$ окажется
в пределах $\limaverage{x}\pm\sigma$ оказывается равна
\[
P_{\left|\Delta x\right|<\sigma} =
\int\limits_{\limaverage{x}-\sigma}^{\limaverage{x}+\sigma}
w_{\mathcal{N}} dx \approx0{,}68.
\]
Вероятность отклонения в пределах $\limaverage{x}\pm2\sigma$:
\[
P_{\left|\Delta x\right|<2\sigma}\approx0{,}95,
\]
а в пределах $\limaverage{x}\pm3\sigma$:
\[
P_{\left|\Delta x\right|<3\sigma}\approx0{,}9973.
\]
Иными словами, при большом числе измерений нормально распределённой
величины можно ожидать, что лишь треть измерений выпадут за пределы интервала
$\left[\bar{x}-\sigma,\,\bar{x}+\sigma\right]$. При этом около 5\%
измерений выпадут за пределы $\left[\bar{x}-2\sigma;\bar{x}+2\sigma\right]$,
и лишь 0,27\% окажутся за пределами
$\left[\bar{x}-3\sigma;\bar{x}+3\sigma\right]$.

\example{В сообщениях об открытии бозона Хиггса на Большом адронном коллайдере
говорилось о том, что исследователи ждали подтверждение результатов
с точностью \textquote{5 сигма}. Используя нормальное распределение \eqref{eq:normal}
нетрудно посчитать, что они использовали доверительную вероятность
$P\approx1-5{,}7\cdot10^{-7}=0{,}99999943$. Такую точность можно назвать фантастической.}

Полученные значения доверительных вероятностей используются при
\emph{стандартной записи результатов измерений}. В физических измерениях
(в частности, в учебной лаборатории), как правило, используется $P=0{,}68$,
то есть, запись
\[
x=\bar{x}\pm\delta x
\]
означает, что измеренное значение лежит в диапазоне (доверительном
интервале) $x\in\left[\bar{x}-\delta x;\bar{x}+\delta x\right]$ с
вероятностью 68\%. Таким образом погрешность $\pm\delta x$ считается
равной одному среднеквадратичному отклонению: $\delta x=\sigma$.
В \emph{технических} измерениях чаще используется $P=0{,}95$, то есть под
абсолютной погрешностью имеется в виду удвоенное среднеквадратичное
отклонение, $\delta x=2\sigma$. Во избежание разночтений доверительную
вероятность следует указывать отдельно.

\note{Хотя нормальный закон распределения встречается на практике довольно
    часто, стоит помнить, что он реализуется \emph{далеко не всегда}.
    Полученные выше соотношения для вероятностей попадания значений в
    доверительные интервалы можно использовать в качестве простейшего
    признака нормальности распределения: в частности, если количество попадающих
    в интервал $\pm \sigma$ результатов существенно отличается от 2/3 --- это повод
    для более детального исследования закона распределения ошибок.}


\paragraph{Сравнение результатов измерений.}
Теперь мы можем дать количественный критерий для сравнения двух измеренных
величин или двух результатов измерения одной и той же величины.

Пусть $x_{1}$ и $x_{2}$ ($x_{1}\ne x_{2}$) измерены с
погрешностями $\sigma_{1}$ и $\sigma_{2}$ соответственно.
Ясно, что если различие результатов $|x_2-x_1|$ невелико,
его можно объяснить просто случайными отклонениями.
Если же теория предсказывает, что вероятность обнаружить такое отклонение
слишком мала, различие результатов следует признать \emph{значимым}.
Предварительно необходимо договориться о соответствующем граничном значении
вероятности. Универсального значения здесь быть не может,
поэтому приходится полагаться на субъективный выбор исследователя. Часто
в качестве \textquote{разумной} границы выбирают вероятность 5\%,
что, как видно из изложенного выше, для нормального распределения
соответствует отклонению более, чем на $2\sigma$.

Допустим, одна из величин известна с существенно большей точностью:
$\sigma_{2}\ll\sigma_{1}$ (например, $x_{1}$ --- результат, полученный
студентом в лаборатории, $x_{2}$ --- справочное значение).
Поскольку $\sigma_{2}$ мало, $x_{2}$ можно принять за \textquote{истинное}:
$x_{2}\approx \limaverage{x}$. Предполагая, что погрешность измерения
$x_{1}$ подчиняется нормальному закону с и дисперсией $\sigma_{1}^{2}$,
можно утверждать, что
% можно с помощью функции (\ref{eq:normal}) вычислить вероятность
% того, что отклонение $\left|x_{1}-x_{2}\right|$ возникло исключительно
% в силу случайных причин.
% То есть
различие считают будет значимы, если
\[
\left|x_{1}-x_{2}\right|>2\sigma_{1}.
\]

Пусть погрешности измерений сравнимы по порядку величины:
$\sigma_{1}\sim\sigma_{2}$. В теории вероятностей показывается, что
линейная комбинация нормально распределённых величин также имеет нормальное
распределение с дисперсией $\sigma^{2}=\sigma_{1}^{2}+\sigma_{2}^{2}$
(см. также правила сложения погрешностей (\ref{eq:sigma_sum})). Тогда
для проверки гипотезы о том, что $x_{1}$ и $x_{2}$ являются измерениями
одной и той же величины, нужно вычислить, является ли значимым отклонение
$\left|x_{1}-x_{2}\right|$ от нуля при $\sigma=\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}$.

\example{Два студента получили следующие значения для теплоты испарения
    некоторой жидкости: $x_{1}=40{,}3\pm0{,}2$~кДж/моль и
    $x_{2}=41{,}0\pm0{,}3$~кДж/моль, где погрешность соответствует
    одному стандартному отклонению. Можно ли утверждать, что они исследовали
    одну и ту же жидкость?\par
Имеем наблюдаемую разность $\left|x_{1}-x_{2}\right|=0{,}7$~кДж/моль,
среднеквадратичное отклонение для разности
$\sigma=\sqrt{0{,}2^{2}+0{,}3^{2}}=0{,}36$~кДж/моль.
Их отношение $\frac{\left|x_{2}-x_{1}\right|}{\sigma}\approx2$. Из
свойств нормального распределения находим вероятность того, что измерялась
одна и та же величина, а различия в ответах возникли из-за случайных
ошибок: $P\approx5\%$. Ответ на вопрос, \textquote{достаточно}
ли мала или велика эта вероятность, остаётся на усмотрение исследователя.}

\note{Изложенные здесь соображения применимы, только если $\limaverage{x}$ и
его стандартное отклонение $\sigma$ получены на основании достаточно
большой выборки $n\gg1$ (или заданы точно). При небольшом числе измерений
($n\lesssim10$) выборочные средние $\average{x}$ и среднеквадратичное отклонение
$s_x$ сами имеют довольно большую ошибку, а
их распределение будет описываться не нормальным законом, а так
называемым $t$-распределением Стъюдента. В частности, в зависимости от
значения $n$ интервал $\average{x}\pm s_{x}$ будет соответствовать несколько
меньшей доверительной вероятности, чем $P=0{,}68$. Особенно резко различия
проявляются при высоких уровнях доверительных вероятностей $P\to1$.
Подробнее об этом см. {[}?{]}.}

% \section{Распределение Пуассона}
% \disclaimer{
%     Распределение Пуассона применяется в случаях, когда имеет место измерения
% количества событий, произошедших за определенный интервал времени или в
% определенном объеме. Понимание этого распределения необходимо для студентов 5
% семестра, изучающих основы физички частиц. Остальные студенты могут пропустить
% этот раздел.
% }
%
% \todo[inline, color = red]{TODO Пуассон}

\section{Независимые величины. Корреляция}
\todo[inline, author=Nozik]{Эта глава вообще зачем нужна?}

Рассмотрим две физические величины $x$ и $y$. Величины называют
\emph{независимыми} если результат измерения одной из них никак не
влияет на результат измерения другой.

Обозначим отклонения от средних как $\Delta x=x-\average{x}$ и $\Delta
y=y-\average{y}$.
Средние значения отклонений равны, очевидно, нулю: $\average{\Delta
x}=x-\average{x}=0$,
$\average{\Delta y}=0$. Из независимости величин $x$ и $y$ следует,
что среднее значение от произведения $\average{\Delta x\cdot\Delta y}$
равно произведению средних $\average{\Delta x}\cdot\average{\Delta y}$
и, следовательно, равно нулю:
\begin{equation}
\average{\Delta x\cdot\Delta y}=\average{\Delta x}\cdot\average{\Delta
y}=0.\label{eq:indep}
\end{equation}

Если $x$ и $y$ не являются независимыми, среднее значение произведения
их отклонений может быть использовано как количественная мера их зависимости.
Наиболее употребительной мерой зависимости двух случайных величин
является \emph{коэффициент линейной корреляции}:
\begin{equation}
r_{xy}=\frac{\average{\Delta x\cdot\Delta
y}}{\sigma_{x}\cdot\sigma_{y}}.\label{eq:pearson}
\end{equation}
Нетрудно проверить (с помощью неравенства Коши\textendash Буняковского),
что $-1\le r\le1$. В частности, для полностью независимых величин
коэффициент корреляции равен нулю, $r=0$, а для линейно зависимых
$y=kx+b$ нетрудно получить $r=1$ при $k>0$ и $r=-1$ при $k<0$.
Примеры промежуточных случаев представлены на рис. TODO.

Если коэффициент $r_{xy}$ близок к единице, говорят, что величины
\emph{коррелируют} между собой (от \emph{англ.} correlate ---
находиться в связи).

\paragraph{Отсутствие корреляции $\protect\not\Rightarrow$ независимость.}

Отметим, что (\ref{eq:indep}) --- необходимое,
но не достаточное условие независимости величин. На рис. TODO приведён
пример очевидно зависимых $x$ и $y$, для которых $r\approx0$.

\paragraph{Корреляция $\protect\not\Rightarrow$ причинность.}

Ещё одна типичная ошибка --- исходя из большого
коэффициента корреляции ($r\to1$) между двумя величинами сделать
вывод о функциональной (причинной) связи между $x$ и $y$. Рассмотрим
конкретный пример. Между током и напряжением на некотором резисторе
имеет место линейная зависимость $U=IR$, и коэффициент корреляции
$r_{UI}$ действительно равен единице. Однако \emph{обратное
в общем случае неверно}. Например, ток в резисторе коррелирует
с его температурой $T$, $r_{IT}\to1$ (больше ток --- больше
тепловыделение по закону Джоуля\textendash Ленца), однако ясно, что
нагрев резистора извне не приведёт к повышению тока в нём (скорее
наоборот, так как сопротивление металлов с температурой растёт). Ошибка
отождествления корреляции и причинности особенно характерна при исследовании
сложных многофакторных систем, например, в медицине, социологии и
т.п.

\section{Дисперсия суммы}\label{sec:sum2}

Пусть измеряемая величина $z=x+y$ складывается из двух \emph{независимы}х
случайных слагаемых $x$ и $y$, для которых известны средние значения
$\average{x}$ и $\average{y}$, и их среднеквадратичные погрешности
$\sigma_{x}$ и $\sigma_{y}$. Непосредственно из определения (\ref{eq:average})
следует вполне очевидный результат, что среднее суммы равно сумме
средних:
\[
    \average{z}=\average{x}+\average{y}.
\]

Найдём дисперсию $\sigma_{z}^{2}$. В силу независимости имеем
\[
    \average{\Delta z^{2}}=\average{\Delta x^{2}}+\average{\Delta
    y^{2}}+2\average{\Delta x\cdot\Delta y}=\average{\Delta x^{2}}+\average{\Delta
    y^{2}},
\]
то есть:
\begin{equation}
    \label{eq:sigma_sum}
    \boxed{{\sigma_{x+y}=\sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}}}}.
\end{equation}
Таким образом, при сложении \emph{независимых }величин их погрешности
складываются среднеквадратичным образом.

Подчеркнём, что для справедливости соотношения (\ref{eq:sigma_sum})
величины $x$ и $y$ не обязаны быть нормально распределёнными ---
достаточно существования конечных значений их дисперсий (однако можно
показать, что если $x$ и $y$ распределены нормально, нормальным
будет и распределение их суммы).

\note{Требование независимости
слагаемых является принципиальным. Например, положим $y=x$. Тогда
$z=2x$. Здесь $y$ и $x$, очевидно, зависят друг от друга. Используя
(\ref{eq:sigma_sum}), находим $\sigma_{2x}=\sqrt{2}\sigma_{x}$,
что, конечно, неверно --- непосредственно из определения
следует, что $\sigma_{2x}=2\sigma_{x}$.}

Отдельно стоит обсудить математическую структуру формулы (\ref{eq:sigma_sum}).
Если если одна из погрешностей много больше другой, например,
$\sigma_{x}\gg\sigma_{y}$,
то меньшей погрешностью можно пренебречь, $\sigma_{x+y}\approx\sigma_{x}$.
С другой стороны, если два источника погрешностей имеют один порядок
$\sigma_{x}\sim\sigma_{y}$, то и $\sigma_{x+y}\sim\sigma_{x}\sim\sigma_{y}$.

Эти обстоятельства важны при планирования эксперимента: как правило,
величина, измеренная наименее точно, вносит наибольший вклад в погрешность
конечного результата. При этом, пока не устранены наиболее существенные
ошибок, бессмысленно гнаться за повышением точности измерения остальных
величин.

\example{Пусть $\sigma_{y}=\sigma_{x}/3$,
тогда $\sigma_{z}=\sigma_{x}\sqrt{1+\frac{1}{9}}\approx1{,}05\sigma_{x}$,
то есть при различии двух погрешностей более, чем в 3 раза, поправка
к погрешности составляет менее 5\%, и уже нет особого смысла в учёте
меньшей погрешности: $\sigma_{z}\approx\sigma_{x}$. Это утверждение
касается сложения любых независимых источников погрешностей в эксперименте.}

\section{Погрешность среднего}\label{sec:average}

Выборочное среднее арифметическое значение $\average{x}$, найденное
по результатам $n$ измерений, само является случайной величиной.
Действительно, если поставить серию одинаковых опытов по $n$ измерений,
то в каждом опыте получится своё среднее значение, отличающееся от
предельного среднего $\limaverage{x}$.

Вычислим среднеквадратичную погрешность среднего арифметического
$\sigma_{\average{x}}$.
Рассмотрим вспомогательную сумму $n$ слагаемых
\[
    Z=x_{1}+x_{2}+\ldots+x_{n}.
\]
Если $\left\{ x_{i}\right\} $ есть набор \emph{независимых} измерений
\emph{одной и той же} физической величины, то мы можем, применяя результат
(\ref{eq:sigma_sum}) предыдущего параграфа, записать
\[
    \sigma_{Z}=\sqrt{\sigma_{x_{1}}^{2}+\sigma_{x_{2}}^{2}+\ldots+\sigma_{x_{n}}^{2}
    }=\sqrt{n}\sigma_{x},
\]
поскольку под корнем находится $n$ одинаковых слагаемых. Отсюда с
учётом $\average{x}=Z/n$ получаем
\begin{equation}
\boxed{{\sigma_{\average{x}}=\frac{\sigma_{x}}{\sqrt{n}}}}.\label{eq:sigma_avg}
\end{equation}

Таким образом, \emph{погрешность среднего значения $x$ по результатам
$n$ независимых измерений оказывается в $\sqrt{n}$ раз меньше погрешности
отдельного измерения}. Это один из важнейших результатов, позволяющий
уменьшать случайные погрешности эксперимента за счёт многократного
повторения измерений.

Подчеркнём отличия между $\sigma_{x}$ и $\sigma_{\average{x}}$:

величина $\sigma_{x}$ --- \emph{погрешность отдельного
измерения} --- является характеристикой разброса значений
в совокупности измерений $\left\{ x_{i}\right\} $, $i=1..n$. При
нормальном законе распределения примерно 68\% измерений попадают в
интервал $\average{x}\pm\sigma_{x}$;

величина $\sigma_{\average{x}}$ --- \emph{погрешность
среднего} --- характеризует точность, с которой определено
среднее значение измеряемой физической величины $\average{x}$ относительно
предельного (\textquote{истинного}) среднего $\limaverage{x}$;
при этом с доверительной вероятностью $P=68\%$ искомая величина $\limaverage{x}$
лежит в интервале
$\average{x}-\sigma_{\average{x}}<\limaverage{x}<\average{x}+\sigma_{\average{x}}$.

% \section{Погрешность погрешности}
% \todo[inline, author = Nozik]{Вот это материал повышенной сложности. Не
% уверен, что он вообще нужен и уж точно не в середине главы}
% \todo[inline,author = ppv]{Сложности тут особой не вижу, но материал
% действительно побочный. Но полезный -- ведь это обоснование того, как нужно
% округлять погрешность.}

% С какой точностью можно вычислить величину $\sigma$ по ограниченному
% количеству $n$ измерений? Оценим среднеквадратичное отклонения от
% своего среднего значения для величины $s$, вычисляемой по формуле
% (\ref{eq:sigma_straight}). Её квадрат $s^{2}$, как нетрудно видеть,
% состоит из $n$ примерно одинаковых слагаемых (обозначим их как
% $\xi_{i}=\left(x_{i}-\average{x}\right)^{2}$):
% \[
% s^{2}=\frac{1}{n-1}\sum_{i}\xi_{i}.
% \]
% Тогда, повторяя рассуждения п. \ref{subsec:average}, приходим к выводу,
% что погрешность вычисления $s^{2}$ пропорциональна корню из числа
% входящих в неё слагаемых (точнее, нужно использовать число \emph{независимых}
% слагаемых, равное $n-1$, как показано выше):
% \[
% \sigma_{s^{2}}\approx\sqrt{n}\cdot\frac{\sigma_{\xi}}{n-1}\approx
% \frac{\sigma_{\xi}}{\sqrt{n-1}}.
% \]
% С учётом того, что $\average{\xi}\approx s^{2}\approx\sigma_{x}^{2}$,
% величину $\sigma_{\xi}=\sqrt{\average{\left(\xi-\average{\xi}\right)^{2}}}$
% можно по порядку величины оценить как
% $\sigma_{\xi}\sim\average{\xi}\sim\sigma_{x}^{2}$;
% точный расчёт с использованием распределения Гаусса (\ref{eq:normal})
% даёт $\sigma_{\xi}=\sqrt{2}\sigma_{x}\approx\sqrt{2}s.$ Наконец,
% из соотношения $\sigma_{s^{2}}=2s\sigma_{s}$ (см. формулу (\ref{eq:sxy})
% ниже), окончательно получаем
% \begin{equation}
% \sigma_{s}=\frac{s}{\sqrt{2\left(n-1\right)}}.\label{eq:sigma_sigma}
% \end{equation}
% Более подробный и аккуратный вывод можно найти, например в {[}?{]}.

% Главный вывод, который можно сделать на основании результата
% (\ref{eq:sigma_sigma})
% --- ошибка вычисления стандартного отклонения, как правило,
% довольно велика. Например, при $n=6$ её относительная величина составляет
% $\approx$30\%, и даже при $n=50$ она уменьшается лишь до $10\%$.
% По этой причине величину погрешности имеет смысл \emph{округлять до
% 1\textendash 2 значащих цифр} (см. также п.~\ref{subsec:round}).

\section{Результирующая погрешность опыта}

Пусть для некоторого результата измерения известна оценка его максимальной
систематической погрешности $\Delta_{\text{сист}}$ и случайная
среднеквадратичная
погрешность $\sigma_{\text{случ}}$. Какова \textquote{полная}
погрешность измерения?

Предположим для простоты, что измеряемая величина \emph{в принципе}
может быть определена сколь угодно точно, так что можно говорить о
некотором её \textquote{истинном} значении $x_{\text{ист}}$
(иными словами, погрешность результата связана в основном именно с
процессом измерения). Назовём \emph{полной погрешностью} измерения
среднеквадратичное значения отклонения от результата измерения от
\textquote{истинного}:
\[
\sigma_{\text{полн}}^{2}=\average{\left(x-x_{\text{ист}}\right)^{2}}.
\]
Отклонение $x-x_{\text{ист}}$ можно представить как сумму случайного
отклонения от среднего $\delta x_{\text{случ}}=x-\limaverage{x}$
и постоянной (но, вообще говоря, неизвестной) систематической составляющей
$\delta x_{\text{сист}}= \limaverage{x} - x_{\text{ист}} = \mathrm{const}$:
\[
x-x_{\text{ист}}=\delta x_{\text{сист}}+\delta x_{\text{случ}}.
\]
Причём случайную составляющую можно считать независимой от систематической.
В таком случае из \eqref{eq:sigma_sum} находим:
\begin{equation}
\sigma_{\text{полн}}^{2}=\average{\delta x_{\text{сист}}^{2}}+\average{\delta
x_{\text{случ}}^{2}}\le\Delta_{\text{сист}}^{2}+\sigma_{\text{случ}}^{2}.
\label{eq:syst_full}
\end{equation}
Таким образом, для получения \emph{максимального} значения полной
погрешности некоторого измерения нужно квадратично сложить максимальную
систематическую и случайную погрешности.

Если измерения проводятся многократно, то согласно (\ref{eq:sigma_avg})
случайная составляющая погрешности может быть уменьшена, а систематическая
составляющая при этом остаётся неизменной:
\[
\sigma_{\text{полн}}^{2}\le\Delta_{\text{сист}}^{2}+\frac{\sigma_{x}^{2}}{n}.
\]

Отсюда следует важное практическое правило
(см. также обсуждение в п.~\ref{sec:sum2}): если случайная погрешность измерений
в 2--3 раза меньше предполагаемой систематической, то
\emph{нет смысла проводить многократные измерения} в попытке уменьшить погрешность
всего эксперимента. В такой ситуации измерения достаточно повторить
2--3 раза --- чтобы убедиться в повторяемости результата, исключить промахи
и проверить, что случайная ошибка действительно мала.
В противном случае повторение измерений может иметь смысл до
тех пор, пока погрешность среднего
$\sigma_{\average{x}}=\frac{\sigma_{x}}{\sqrt{n}}$
не станет меньше систематической.

\note{Поскольку конкретная
величина систематической погрешности, как правило, не известна, её
можно в некотором смысле рассматривать наравне со случайной ---
предположить, что её величина была определена по некоторому случайному
закону перед началом измерений (например, при изготовлении линейки
на заводе произошло некоторое случайное искажение шкалы). При такой
трактовке формулу \eqref{eq:syst_full} можно рассматривать просто
как частный случай формулы сложения погрешностей независимых величин
\eqref{eq:sigma_sum}.\par
Подчеркнем, что вероятностный закон, которому подчиняется
систематическая ошибка, зачастую неизвестен. Поэтому неизвестно и
распределение итогового результата. Из этого, в частности, следует,
что мы не может приписать интервалу $x\pm\Delta_{\text{сист}}$ какую-либо
определённую доверительную вероятность --- она равна 0,68
только если систематическая ошибка имеет нормальное распределение.
Можно, конечно, \emph{предположить},
--- и так часто делают --- что, к примеру, ошибки
при изготовлении линеек на заводе имеют гауссов характер. Также часто
предполагают, что систематическая ошибка имеет \emph{равномерное}
распределение (то есть \textquote{истинное} значение может с равной вероятностью
принять любое значение в пределах интервала $\pm\Delta_{\text{сист}}$).
Строго говоря, для этих предположений нет достаточных оснований.}

\example{В результате измерения диаметра проволоки микрометрическим винтом,
имеющим цену деления $h=0,01$ мм, получен следующий набор из $n=8$ значений:\par
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
$d$, мм & 0,39 & 0,38 & 0,39 & 0,37 & 0,40 & 0,39 & 0,38 & 0,39 \\ \hline
\end{tabular}\par}
\smallskip
Вычисляем среднее значение: $\average{d}\approx386{,}3$~мкм.
Среднеквадратичное отклонение:
% вычисляем по формуле (\ref{eq:sigma_straight}):
$\sigma_{d}\approx9{,}2$~мкм. Случайная погрешность среднего согласно
(\ref{eq:sigma_avg}):
$\sigma_{\average{d}}=\frac{\sigma_{d}}{\sqrt{8}}\approx3{,}2$
мкм. Все результаты лежат в пределах $\pm2\sigma_{d}$, поэтому нет
причин сомневаться в нормальности распределения. Максимальную погрешность
микрометра оценим как половину цены деления, $\Delta=\frac{h}{2}=5$~мкм.
Результирующая полная погрешность
$\sigma\le\sqrt{\Delta^{2}+\frac{\sigma_{d}^{2}}{8}}\approx6{,}0$~мкм.
Видно, что $\sigma_{\text{случ}}\approx\Delta_{\text{сист}}$ и проводить дополнительные измерения
особого смысла нет. Окончательно результат измерений может быть представлен
в виде (см. также \emph{правила округления}
результатов измерений в п.~\ref{subsec:round})
\[
d=386\pm6\;\text{мкм},\qquad\varepsilon_{d}=1{,}5\%.
\]

Заметим, что поскольку случайная погрешность и погрешность
прибора здесь имеют один порядок величины, наблюдаемый случайный разброс
данных может быть связан как с неоднородностью сечения проволоки,
так и с дефектами микрометра (например, с неровностями зажимов, люфтом
винта, сухим трением, деформацией проволоки под действием микрометра
и т.\,п.). Для ответа на вопрос, что именно вызвало разброс, требуются
дополнительные исследования, желательно с использование более точных
приборов.\par
}%\footnotesize

\example{Измерение скорости
полёта пули было осуществлено с погрешностью $\delta v=\pm1$ м/c.
Результаты измерений для $n=6$ выстрелов представлены в таблице:\par
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$v$, м/с & 146 & 170 & 160 & 181 & 147 & 168 \\ \hline
\end{tabular}\par}
\smallskip
Усреднённый результат $\average{v}=162{,}0\;\text{м/с}$,
среднеквадратичное отклонение $\sigma_{v}=13{,}8\;\text{м/c}$, случайная
ошибка для средней скорости
$\sigma_{\bar{v}}=\sigma_{v}/\sqrt{6}=5{,}6\;\text{м/с}$.
Поскольку разброс экспериментальных данных существенно превышает погрешность
каждого измерения, $\sigma_{v}\gg\delta v$, он почти наверняка связан
с реальным различием скоростей пули в разных выстрелах, а не с ошибками
измерений. В качестве результата эксперимента представляют интерес
как среднее значение скоростей $\average{v}=162\pm6\;\text{м/с}$
($\varepsilon\approx4\%$), так и значение $\sigma_{v}\approx14\;\text{м/с}$,
характеризующее разброс значений скоростей от выстрела к выстрелу.
Малая инструментальная погрешность в принципе позволяет более точно
измерить среднее и дисперсию, и исследовать закон распределения выстрелов
по скоростям более детально --- для этого требуется набрать
б\'{о}льшую статистику по выстрелам.\par
}%\footnotesize

\example{Измерение скорости
полёта пули было осуществлено с погрешностью $\delta v=10$ м/c. Результаты
измерений для $n=6$ выстрелов представлены в таблице:\par
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$v$, м/с & 150 & 170 & 160 & 180 & 150 & 170 \\ \hline
\end{tabular}\par}
\smallskip
Усреднённый результат $\average{v}=163{,}3\;\text{м/с}$,
$\sigma_{v}=12{,}1\;\text{м/c}$, $\sigma_{\average{v}}=5\;\text{м/с}$,
$\sigma_{\text{полн}}\approx11{,}2\;\text{м/с}$. Инструментальная
погрешность каждого измерения превышает разброс данных, поэтому в
этом опыте затруднительно сделать вывод о различии скоростей от выстрела
к выстрелу. Результат измерений скорости пули:
$\average{v}=163\pm11\;\text{м/с}$,
$\varepsilon\approx7\%$. Проводить дополнительные выстрелы при такой
большой инструментальной погрешности особого смысла нет ---
лучше поработать над точностью приборов и методикой измерений.\par
}%\footnotesize

\section{Обработка косвенных измерений\label{sec:kosv}}

\emph{Косвенными} называют измерения, полученные в результате расчётов,
использующих результаты \emph{прямых} (то есть \textquote{непосредственных})
измерений физических величин. Сформулируем основные правила пересчёта
погрешностей при косвенных измерениях.

\subsection{Случай одной переменной}

Пусть в эксперименте измеряется величина $x$, а её \textquote{наилучшее}
(в некотором смысле) значение равно $x^{\star}$ и оно известно с
погрешностью $\sigma_{x}$. После чего с помощью известной функции
вычисляется величина $y=f\!\left(x\right)$.

В качестве \textquote{наилучшего} приближения для $y$ используем значение функции
при \textquote{наилучшем} $x$:
\[
y^{\star}=f\!\left(x^{\star}\right).
\]

Найдём величину погрешности $\sigma_{y}$. Обозначая отклонение измеряемой
величины как $\Delta x=x-x^{\star}$, и пользуясь определением производной,
при условии, что функция $y\left(x\right)$ --- гладкая
вблизи $x\approx x^{\star}$, запишем
\[
\Delta y\equiv y\left(x\right)-y\left(x^{\star}\right)\approx f'\cdot\Delta x,
\]
где $f'\equiv\frac{dy}{dx}$ --- производная фукнции $f(x)$, взятая в точке
$x^{\star}$. Возведём полученное в квадрат, проведём усреднение
($\sigma_{y}^{2}=\average{\Delta y^{2}}$,
$\sigma_{x}^{2}=\average{\Delta x^{2}}$), и затем снова извлечём
корень. В результате получим
\begin{equation}
\boxed{{\sigma_{y}=\left|\frac{dy}{dx}\right|\sigma_{x}.}}\label{eq:sxy}
\end{equation}

\example{Для степенной функции
$y=Ax^{n}$ имеем $\sigma_{y}=nAx^{n-1}\sigma_{x}$, откуда
\[
\frac{\sigma_{y}}{y}=n\frac{\sigma_{x}}{x},\qquad\text{или}\qquad\varepsilon_{y}
=n\varepsilon_{x},
\]
то есть относительная погрешность степенной функции возрастает пропорционально
показателю степени $n$.\par
}%\footnotesize

\example{Для $y=1/x$ имеем $\varepsilon_{1/x}=\varepsilon_{x}$
--- при обращении величины сохраняется её относительная
погрешность.\par
}%\footnotesize

\exercise{Найдите погрешность логарифма $y=\ln x$, если известны~$x$
    и~$\sigma_{x}$.}

\exercise{Найдите погрешность показательной функции $y=a^{x}$,
    если известны~$x$ и~$\sigma_{x}$. Коэффициент $a$ задан точно.}


\subsection{Случай многих переменных}

Пусть величина $u$ вычисляется по измеренным значениям нескольких
различных \emph{независимых} физических величин $x$, $y$, $\ldots$
на основе известного закона $u=f\!\left(x,y,\ldots\right)$. В качестве
наилучшего значения можно по-прежнему взять значение функции $f$
при наилучших значениях измеряемых параметров:
\[
u^{\star}=f\!\left(x^{\star},y^{\star},\ldots\right).
\]

Для нахождения погрешности $\sigma_{u}$ воспользуемся свойством,
известным из математического анализа, --- малые приращения гладких
функции многих переменных складываются линейно, то есть справедлив
\emph{принцип суперпозиции} малых приращений:
\[
\Delta u\approx f'_{x}\cdot\Delta x+f'_{y}\cdot\Delta y+\ldots,
\]
где символом $f'_{x}\equiv\frac{\partial f}{\partial x}$ обозначена
\emph{частная производная} функции $f$ по переменной $x$ ---
то есть обычная производная $f$ по $x$, взятая при условии, что
все остальные аргументы (кроме $x$) считаются постоянными параметрами.
Тогда пользуясь формулой для нахождения дисперсии суммы независимых
величин (\ref{eq:sigma_sum}), получим соотношение, позволяющее вычислять
погрешности косвенных измерений для произвольной функции
$u=f\left(x,y,\ldots\right)$:
\begin{equation}
\boxed{\sigma_{u}^{2}=f_{x}^{\prime2}\,\sigma_{x}^{2}+f_{y}^{\prime2}\,\sigma_{y
}^{2}+\ldots}\label{eq:sigma_general}
\end{equation}
Это и есть искомая общая формула пересчёта погрешностей при косвенных
измерениях.

Отметим, что формулы (\ref{eq:sxy}) и (\ref{eq:sigma_general}) применимы
только если относительные отклонения всех величин малы
($\varepsilon_{x},\varepsilon_{y},\ldots\ll1$),
а измерения проводятся вдали от особых точек функции $f$ (производные
$f_{x}'$, $f_{y}'$ $\ldots$ не должны обращаться в бесконечность).
Также подчеркнём, что все полученные здесь формулы справедливы только
для \emph{независимых} переменных $x$, $y$, $\ldots$

Остановимся на некоторых важных частных случаях формулы
(\ref{eq:sigma_general}).

\example{Для суммы (или разности) $u=\sum\limits _{i=1}^{n}a_{i}x_{i}$ имеем
\begin{equation}
\sigma_{u}^{2}=\sum_{i=1}^{n}a_{i}^{2}\sigma_{x_{i}}^{2}.
\end{equation}}

\example{Найдём погрешность степенной функции:
    $u=x^{\alpha}\cdot y^{\beta}\cdot\ldots$. Тогда нетрудно получить,
что
\[
\frac{\sigma_{u}^{2}}{u^{2}}=\alpha^{2}\frac{\sigma_{x}^{2}}{x^{2}}+\beta^{2}
\frac{\sigma_{y}^{2}}{y^{2}}+\ldots
\]
или через относительные погрешности
\begin{equation}
\varepsilon_{u}^{2}=\alpha^{2}\varepsilon_{x}^{2}+\beta^{2}\varepsilon_{y}^{2}
+\ldots\label{eq:espilon_power}
\end{equation}}

\example{Вычислим погрешность произведения и частного: $u=xy$ или $u=x/y$.
    Тогда в обоих случаях имеем
\begin{equation}
\varepsilon_{u}^{2}=\varepsilon_{x}^{2}+\varepsilon_{y}^{2},
\end{equation}
то есть при умножении или делении относительные погрешности складываются
квадратично.}

\example{Рассмотрим несколько более сложный случай: нахождение угла по его тангенсу
\[
u=\arctg\frac{y}{x}.
\]
В таком случае, пользуясь тем, что $\left(\arctg z\right)'=\frac{1}{1+z^{2}}$,
где $z=y/x$, и используя производную сложной функции, находим
$u_{x}'=u_{z}'z'_{x}=-\frac{y}{x^{2}+y^{2}}$,
$u_{y}'=u'_{z}z'_{y}=\frac{x}{x^{2}+y^{2}}$, и наконец
\[
\sigma_{u}^{2}=\frac{y^{2}\sigma_{x}^{2}+x^{2}\sigma_{y}^{2}}{\left(x^{2}+y^{2}
\right)^{2}}.
\]}

\exercise{Найти погрешность вычисления гипотенузы $z=\sqrt{x^{2}+y^{2}}$
    прямоугольного треугольника по измеренным катетам $x$ и $y$.}

По итогам данного раздела можно дать следующие практические рекомендации.
\begin{itemize}
\item Как правило, нет смысла увеличивать точность измерения какой-то одной
величины, если другие величины, используемые в расчётах, остаются
измеренными относительно грубо --- всё равно итоговая погрешность
скорее всего будет определяться самым неточным измерением. Поэтому
все измерения имеет смысл проводить \emph{примерно с одной и той же
относительной погрешностью}.
\item При этом, как следует из (\ref{eq:espilon_power}), особое внимание
следует уделять измерению величин, возводимых при расчётах в степени
с большими показателями. А при сложных функциональных зависимостях
имеет смысл детально проанализировать структуру формулы
(\ref{eq:sigma_general}):
если вклад от некоторой величины в общую погрешность мал, нет смысла
гнаться за высокой точностью её измерения, и наоборот, точность некоторых
измерений может оказаться критически важной.
\item Следует избегать измерения малых величин как разности двух близких
значений (например, толщины стенки цилиндра как разности внутреннего
и внешнего радиусов): если $u=x-y$, то абсолютная погрешность
$\sigma_{u}=\sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}}$
меняется мало, однако относительная погрешность
$\varepsilon_{u}=\frac{\sigma_{u}}{x-y}$
может оказаться неприемлемо большой, если $x\approx y$.
\end{itemize}
