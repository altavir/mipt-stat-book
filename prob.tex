\chapter{Базовые понятия теории вероятностей}

\label{ch:prob}

\disclaimer{
    Этот раздел рекомендуется для прочтения студентам, в программу которых на младших курсах не входит теория вероятностей. 
}

Определение понятия вероятности является сложной задачей без однозначно правильного решения. В зависимости от области применения, могут применяться различные, во многих случаях взаимоисключающие определения. При этом большинство определений приводят к одним и тем же практическим выводам, поэтому с прикладной точки зрения не так важно, какое из них выбрать. 

\section{Определение случайной величины}
\disclaimer{
    Конкретное определение вероятности не является существенным для практических применений, поэтому всем, кому не интересна идейная составляющая вопроса рекомендуется пропустить этот раздел и пользоваться "наивным" определением, основанном на "здравом смысле".
}

Как это ни парадоксально, формальная теория вероятности и математическая статистика никак не определяют понятие вероятности. Все теоретические выводы основываются на том, что изучаемые величины (вероятности) удовлетворяют некоторым формальным правилам (например системе аксиом Колмогорова \todo{ссылка}), но никаким образом не определяют как эти величины получить. Со времен создания теории вероятности существует два лагеря статистиков, каждый из которых использует свое определение. Обсуждение этих формальных моментов лежит далеко за пределами данного пособия, так что для упрощения понимания материала а также сокращения многих доказательств, в данном пособии мы будем придерживаться следующих определений:

\emph{Случайной} будем называть величину, значение которой не может быть \emph{достоверно} определено экспериментатором с абсолютной точностью или значение которой меняется в каждом последующем опыте неконтролируемым образом. Согласно такому определению, любой результат любых измерений является случайным, поскольку осведомленность экспериментатора всегда чем-то ограничена.
\todo[inline,author=ppv]{Все физические величины случайны? Нужно переформулировать не как определение "случайности", а как утверждение: все физические величины в том или ином смысле случайны.}

\section{Распределения}

Для работы со случайными величинами необходимо понимать, что они не являются полностью произвольными, а подчиняются некоторым законам. Эти законы называются распределениями.

\emph{Распределением случайной величины} $x$ или \emph{функцией плотности вероятности} называют такую функцию $f(x)$, что вероятность для нее находиться в диапазоне от $x$ до $x+dx$ равна $p(x \in (x_0, x_0+dx)) = f(x_0) dx$. Очевидно
\todo{не очевидно}, что такое определение годится только для численным величин и имеет ряд других ограничений, но в данном пособии мы ограничимся им. Более строгие определения можно почерпнуть из специальной литературы (\todo{ссылка}).
\todo[inline,author=ppv]{Отослать один раз в начале главы к более строгим определениям в специальной литературе. Мы заведомо не претендуем на строгость или полноту.}

Важно отметить, что в работах по статистике, написанных математиками, распределением часто называют не саму функцию $f$, а ее интеграл $F(x_0) = \int{f(x)dx}$, который можно определить, не используя бесконечно малые. Такую функцию физики называют \emph{интегральным распределением}.

Обозначим через $P\!\left(\left|\Delta x\right|<\delta\right)$ вероятность
(долю случаев) того, что отклонение от среднего $\Delta x=x-x_{0}$
составит величину, не превосходящую по модулю значение $\delta$:
\begin{equation*}
P\left(\left|\Delta x\right|<\delta\right)=\int\limits _{x_{0}-\delta}^{x_{0}+\delta}w\!\left(x\right)dx.
\end{equation*}

Величину $P$ называют \emph{доверительной вероятностью} для \emph{доверительного интервала} $\left|x-x_{0}\right|\le\delta$. 

Согласно определению вероятности, сумма вероятностей для всех возможных случаев всегда равна единице. Как следствие интеграл распределения $f$ в бесконечных пределах также всегда должен быть равен единицы. Это называют условием нормировки.

\todo[inline,author=ppv]{Понятие плотности вероятности наиболее наглядно изображается именно с помощью гистограммы. Нужно сначала объяснить, что такое гистограмма, а потом вводить определение плотности.}

Для визуализации распределений удобно использовать специальный тип графика: \emph{Гистограмму}.
Область значений $x$, размещённую на оси абсцисс, разобьём на равные малые интервалы --- <<корзины>>
или <<бины>> (\emph{англ.} bins) некоторого
размера $h$. По оси ординат будет откладывать долю измерений $w$,
результаты которых попадают в соответствующую корзину. Например, если
$n_{k}$ измерений лежит в диапазоне $kh<x<(k+1)h$, $k$ ---
номер корзины, то на графике изобразим столбик шириной $h$ и высотой
$w_{k}=n_{k}/n$. В результате получим картину, подобную изображённой
на рис.~\ref{fig:normhist}. 

\begin{figure}
    \centering
    \includegraphics[width=7cm]{images/normhist.pdf}
    \label{fig:normhist}
    \caption{Пример гистограммы для нормального распределения ($\average{x}=10$, $\sigma=1{,}0$, $h=0{,}1$, $n=10^{4}$)}
\end{figure}

В большинстве случаев мы ожидаем, что наиболее вероятное значение распределения соответствует его среднему значению, то есть самые высокие столбики будут группироваться в районе <<середины>> гистограммы. Хотя следует понимать, что это утверждение соблюдается не всегда. Ширина гистограммы будет характеризовать разброс значений распределения.

\example{
    Пусть есть набор резисторов из одной серии с одной и той же маркировкой, соответствующей одному и тому же номиналу их сопротивления. В силу неидеальности процесса изготовления, точное значение резистора является случайной величиной, средней значение которое должно совпадать с заводским номиналом, но при этом будет наблюдаться также некоторый разброс значений. Рассмотрим два измерения: в первом будем просто брать все резисторы из одной серии, замерять их сопротивления и строить гистограмму результата измерений. В этом случае мы увидим картину похожую на рис.~\ref{fig:normhist}.
    Теперь предположим, что перед тем как мы начали проводить свои измерения, кто-то взял и отобрал из изучаемой партии все резисторы с сопротивлением максимально приближенным к номинальному значению. В этом случае окажется, что в нашем измерении вероятность получить сопротивление, близкое к номиналу, будет мала. Как следствие, в результатем измерения будет получено <<двух-горбое>> распределение с провалом посередине. Среднее значение будет таким же, как и в первом случае, но разброс будет больше и наиболее вероятное значение (точнее два наиболее вероятных значения) не будут совпадать со средним.
}

Формальное определение среднего и разброса величины $x$, подчиняющейся распределению $f(x)$ выглядит таким образом:
\begin{equation}
    \average{x} = \int{x f(x) dx}
    \sigma^2 = \int{(x - \average{x})^2 f(x) dx}
\end{equation}

Среднее значение $\average{x}$ также называют \emph{математическим ожиданием} величины по распределению. Величину $\sigma$ называют \emph{среднеквадратичным отклонением}. Срекднеквадратичное отклонение имеет ту же размерность, что и сама величина $x$ и характеризует разброс распределения. Именно эту величину как правило приводят как характеристику погрешности. Величину $\sigma^2$ называют \emph{дисперсией} распределения.


% \disclaimer{
%     Этот раздел предназначен для  изучения методики проведения и анализа физического эксперимента на 2-3 курсах.
% }

% Предположим, что систематические погрешности малы и займёмся отдельно
% изучением случайных погрешностей. Пусть по результатам многократных
% измерений получен набор значений $\left\{ x_{i}\right\} $, вычислено
% их среднее (\ref{eq:average}) $\average{x}$ и среднеквадратичное
% отклонение (\ref{eq:sigma}) $\sigma_{x}\approx s{}_{x}$. Можно надеяться,
% что измеряемая величина лежит в диапазоне
% \[
% x\in\left(\average{x}-\sigma_{x};\,\average{x}+\sigma_{x}\right).
% \]
% Какова вероятность $P$ того, что результат действительно находится
% в указанном интервале?

% Для ответа на этот вопрос необходимо знать \emph{вероятностный закон},
% которому подчиняется исследуемая величина. Казалось бы, для каждой
% случайной физической величины должен существовать свой особенный закон
% и общую теорию здесь построить невозможно. Это отчасти верно, но оказывается,
% что существует вполне \emph{универсальный} вероятностный закон, называемый
% \emph{нормальным}, которому подчиняются многие случайные величины.
% Рассмотрим его подробнее.

\section{Нормальное распределение}

Одним из наиболее примечательных результатов теории вероятностей является
так называемая \emph{центральная предельная теорема}, которая утверждает,
что сумма большого количества независимых случайных слагаемых, каждое
из которых вносит в эту сумму относительно малый вклад, подчиняется
универсальному закону, не зависимо от того, каким вероятностным законам
подчиняются её составляющие, --- так называемому \emph{нормальному
распределению} (или \emph{распределению Гаусса}). Эта теорема имеет
прямое отношение к физическим измерениям, поскольку зачастую случайные
погрешности складываются из множества случайных \emph{независимых}
факторов.

Доказательство теоремы довольно громоздко и мы его не приводим (его можно найти
в \emph{любом} учебнике по теории вероятностей). Остановимся
кратко на том, что такое нормальное распределение и его основных свойствах.

Плотность нормального распределения выражается следующей формулой:
\begin{equation}
    \label{eq:normal}
    \boxed{
        w_{\mathcal{N}}\!\left(x\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\tfrac{(x-x_{0})^{2}}{2\sigma^{2}}}
    }.
\end{equation}
Здесь $x_{0}$ и $\sigma$
--- параметры нормального распределения: $x_{0}$ равно
среднему (и наиболее вероятному) значению $x$, a $\sigma$ ---
среднеквадратичному отклонению, вычисленным в пределе $n\to\infty$. 

Вероятность получить в результате эксперимента величину в диапазоне от $a$ до $b$ или вероятностное содержание интервала $(a,b)$ можно найти взяв соответствующий интеграл:
\begin{equation}
    P_{a\le x\le b}=\int\limits _{a}^{b}w\!\left(x\right)dx.\label{eq:P}
\end{equation}
\todo[inline,author=ppv]{Это в раздел выше}

Как видно из рис.~\ref{fig:normhist}, распределение представляет собой симметричный
<<колокол>>, положение вершины которого
соответствует среднему значению $\bar{x}$ (ввиду симметрии, оно же
совпадает с наиболее вероятным значением --- максимумом
функции $w_{\mathcal{N}}(x)$).

% \begin{figure}
%     \centering
%     \input{gauss.pdf_t}
%     \caption{Плотность нормального распределения}
% \end{figure}


При значительном отклонении $x$ от среднего величина $w_{\mathcal{N}}\!\left(x\right)$
быстро убывает. Это означает, что вероятность встретить отклонения,
существенно большие, чем $\sigma$, оказывается \emph{пренебрежимо
мала}. Ширина <<колокола>> по порядку величины
равна $\sigma$ --- она характеризует <<разброс>>
экспериментальных данных относительно среднего значения.

\note{
    А именно, точки $x=\bar{x}\pm\sigma$ являются точками
    перегиба графика $w\left(x\right)$ (в них вторая производная по $x$
    обращается в нуль, $w''=0$), а их положение по высоте составляет
    $w\!\left(\bar{x}\pm\sigma\right)/w(\bar{x})=e^{_{-1/2}}\approx0{,}61$
    от высоты вершины.
}

Универсальный характер центральной предельной теоремы позволяет широко
применять на практике нормальное (гауссово) распределение для обработки
случайных погрешностей. Заметим, что на практике для \emph{приближённой
оценки} параметров нормального распределения случайной величины используются
\emph{выборочные} значения среднего и дисперсии: $x_{0}\approx\average{x}$,
$s_{x}\approx\sigma_{x}$.

Для нормального распределения доверительные вероятности могут быть
легко найдены численно\footnote{Заметим, что интеграл $\int e^{-x^{2}/2}dx$, называемый \emph{интегралом
ошибок}, в элементарных функциях не выражается.}. Вероятность отклонения в пределах $\pm\sigma$ оказывается равна
\[
P\!\left(\left|\Delta x\right|<\sigma\right)\approx0{,}68,
\]
в пределах $\pm2\sigma$:
\[
P\!\left(\left|\Delta x\right|<2\sigma\right)\approx0{,}95,
\]
а в пределах $\pm3\sigma$:
\[
P\left(\left|\Delta x\right|<3\sigma\right)\approx0{,}9973.
\]
Таким образом, при большом числе измерений нормально распределённой
величины можно ожидать, например, что 68\% измерений попадут в интервал
$\left[\bar{x}-\sigma;\bar{x}+\sigma\right]$. При этом около 5\%
измерений выпадут за пределы $\left[\bar{x}-2\sigma;\bar{x}+2\sigma\right]$,
и лишь 0,27\% окажутся за пределами $\left[\bar{x}-3\sigma;\bar{x}+3\sigma\right]$.
Вероятность ещё больших отклонений очень быстро убывает\footnote{В сообщениях об открытии бозона Хиггса на Большом адронном коллайдере
говорилось о том, что исследователи ждали подтверждение результатов
с точностью <<5 сигма>>, ---
это означает, что они использовали доверительную вероятность $P\approx1-5{,}7\cdot10^{-7}=0{,}99999943$.
Такую точность можно назвать фантастической.}.

Полученные значения доверительных вероятностей используются при \emph{стандартной
записи результатов измерений}. В физических измерениях (в частности,
в учебной лаборатории), как правило, используется $P=0{,}68$, то
есть, запись 
\[
x=\bar{x}\pm\delta x
\]
означает, что измеренное значение лежит в диапазоне (доверительном
интервале) $x\in\left[\bar{x}-\delta x;\bar{x}+\delta x\right]$ с
вероятностью 68\%. Таким образом погрешность $\pm\delta x$ считается
равной одному среднеквадратичному отклонению: $\delta x=\sigma$.
В технических измерениях чаще используется $P=0{,}95$, то есть под
абсолютной погрешностью имеется в виду удвоенное среднеквадратичное
отклонение, $\delta x=2\sigma$. Во избежание разночтений доверительную
вероятность следует указывать отдельно.

\paragraph{Сравнение результатов измерений.}

Теперь мы можем дать количественный критерий для сравнения двух измеренных
величин или двух результатов измерения одной и той же величины.

Пусть величины $x_{1}$ и $x_{2}$ ($x_{1}\ne x_{2}$) измерены с
погрешностями $\sigma_{1}$ и $\sigma_{2}$ соответственно. 

Рассмотрим сперва случай, когда одна из величин известна с существенно
большей точностью: $\sigma_{2}\ll\sigma_{1}$ (например, $x_{1}$
--- результат, полученный студентом в лаборатории, $x_{2}$
--- справочное значение). Ясно, что если $\left|x_{1}-x_{2}\right|\gg\sigma_{1}$
то результаты заведомо не совпадают, а если $\left|x_{1}-x_{2}\right|\lesssim\sigma_{1}$,
то различия между ними можно объяснить случайным отклонением.

Поскольку $\sigma_{2}$ мало, значение $x_{2}$ можно принять за <<истинное>>
среднее и предположить, что погрешность измерения $x_{1}$ подчиняется
нормальному закону (со средним $x_{2}$ и дисперсией $\sigma_{1}^{2}$).
Тогда с помощью функции (\ref{eq:normal}) можно вычислить вероятность
$P\left(\left|x_{1}-x_{2}\right|\right)$ того, что отклонение $\left|x_{1}-x_{2}\right|$
возникло исключительно в силу случайных причин. Предварительно необходимо
договориться о том, какую вероятность отклонения следует считать \emph{значимой}.
Универсального значения здесь быть не может, поэтому приходится полагаться
на субъективный выбор конкретного исследователя. Часто в качестве
границы выбирают вероятность $P=0{,}05$, то есть результаты признаются
различными, если вероятность случайного отклонения меньше 5\%, и совпадающими
в противоположном случае. Как следует из свойств нормального распределения,
рассмотренных выше, такая вероятность соответствует отклонению на
$2\sigma$, то есть различие значимо, если 
\[
\left|x_{1}-x_{2}\right|>2\sigma_{1}.
\]

Пусть теперь погрешности двух измерений сравнимы по порядку величины,
$\sigma_{1}\sim\sigma_{2}$. В теории вероятностей показывается, что
линейная комбинация нормально распределённых величин также имеет нормальное
распределение. Следовательно, разность $x_{1}-x_{2}$ распределена
нормально с дисперсией $\sigma^{2}=\sigma_{1}^{2}+\sigma_{2}^{2}$
(см. также правила сложения погрешностей (\ref{eq:sigma_sum})). Тогда
для проверки гипотезы о том, что $x_{1}$ и $x_{2}$ являются измерениями
одной и той же величины, нужно вычислить, является ли значимым отклонение
$\left|x_{1}-x_{2}\right|$ от нуля.

\example{ 
Два студента получили
следующие значения для теплоты испарения некоторой жидкости: $x_{1}=40{,}3\pm0{,}2$
кДж/моль и $x_{2}=41{,}0\pm0{,}3$ кДж/моль, где погрешность соответствует
одному стандартному отклонению. Можно ли утверждать, что они исследовали
одну и ту же жидкость?

Имеем наблюдаемую разность $\left|x_{1}-x_{2}\right|=0{,}7$,
среднеквадратичное отклонение для разности $\sigma=\sqrt{0{,}2^{2}+0{,}3^{2}}=0{,}36$.
Их отношение $\frac{\left|x_{2}-x_{1}\right|}{\sigma}\approx2$. Из
свойств нормального распределения находим вероятность того, что измерялась
одна и та же величина, а различия в ответах возникли из-за случайных
ошибок: $P\approx5\%$. Ответ на вопрос, <<достаточно>>
ли мала или велика эта вероятность остаётся на усмотрение исследователя.
}

\note{
Изложенные здесь
соображения применимы, только если измеренное значение $x$ и его
стандартное отклонение $\sigma$ получены на основании достаточно
большой выборки $n\gg1$ (или заданы точно). Если приходится иметь
дело с небольшим количеством измерений ($n\lesssim10$), выборочные
средние и стандартные отклонения сами имеют довольно большую погрешность.
Тогда отклонения выборочных величин будут описываться не нормальным
распределением, а так называемым $t$-распределением Стъюдента. В
частности, в зависимости от значения $n$ интервал $\average{x}\pm s_{x}$
будет соответствовать несколько меньшей доверительной вероятности,
чем $P=0{,}68$. Особенно резко различия проявляются при высоких уровнях
доверительных вероятностей $P\to1$. Подробнее об этом см. {[}?{]}.
}

\section{Распределение Пуассона}
\disclaimer{
    Распределение Пуассона применяется в случаях, когда имеет место измерения количества событий, произошедших за определенный интервал времени или в определенном объеме. Понимание этого распределения необходимо для студентов 5 семестра, изучающих основы физички частиц. Остальные студенты могут пропустить этот раздел.
}

\todo[inline, color = red]{TODO Пуассон}

\section{Независимые случайные величины. Корреляция}

Рассмотрим две физические величины $x$ и $y$. Величины называют
\emph{независимыми} если результат измерения одной из них никак не
влияет на результат измерения другой.

Обозначим отклонения от средних как $\Delta x=x-\average{x}$ и $\Delta y=y-\average{y}$.
Средние значения отклонений равны, очевидно, нулю: $\average{\Delta x}=x-\average{x}=0$,
$\average{\Delta y}=0$. Из независимости величин $x$ и $y$ следует,
что среднее значение от произведения $\average{\Delta x\cdot\Delta y}$
равно произведению средних $\average{\Delta x}\cdot\average{\Delta y}$
и, следовательно, равно нулю: 
\begin{equation}
\average{\Delta x\cdot\Delta y}=\average{\Delta x}\cdot\average{\Delta y}=0.\label{eq:indep}
\end{equation}

Если $x$ и $y$ не являются независимыми, среднее значение произведения
их отклонений может быть использовано как количественная мера их зависимости.
Наиболее употребительной мерой зависимости двух случайных величин
является \emph{коэффициент линейной корреляции}:
\begin{equation}
r_{xy}=\frac{\average{\Delta x\cdot\Delta y}}{\sigma_{x}\cdot\sigma_{y}}.\label{eq:pearson}
\end{equation}
Нетрудно проверить (с помощью неравенства Коши\textendash Буняковского),
что $-1\le r\le1$. В частности, для полностью независимых величин
коэффициент корреляции равен нулю, $r=0$, а для линейно зависимых
$y=kx+b$ нетрудно получить $r=1$ при $k>0$ и $r=-1$ при $k<0$.
Примеры промежуточных случаев представлены на рис. TODO.

Если коэффициент $r_{xy}$ близок к единице, говорят, что величины
\emph{коррелируют} между собой (от \emph{англ.} correlate ---
находиться в связи).

\paragraph{Отсутствие корреляции $\protect\not\Rightarrow$ независимость.}

Отметим, что (\ref{eq:indep}) --- необходимое,
но не достаточное условие независимости величин. На рис. TODO приведён
пример очевидно зависимых $x$ и $y$, для которых $r\approx0$.

\paragraph{Корреляция $\protect\not\Rightarrow$ причинность.}

Ещё одна типичная ошибка --- исходя из большого
коэффициента корреляции ($r\to1$) между двумя величинами сделать
вывод о функциональной (причинной) связи между $x$ и $y$. Рассмотрим
конкретный пример. Между током и напряжением на некотором резисторе
имеет место линейная зависимость $U=IR$, и коэффициент корреляции
$r_{UI}$ действительно равен единице. Однако \emph{обратное
в общем случае неверно}. Например, ток в резисторе коррелирует
с его температурой $T$, $r_{IT}\to1$ (больше ток --- больше
тепловыделение по закону Джоуля\textendash Ленца), однако ясно, что
нагрев резистора извне не приведёт к повышению тока в нём (скорее
наоборот, так как сопротивление металлов с температурой растёт). Ошибка
отождествления корреляции и причинности особенно характерна при исследовании
сложных многофакторных систем, например, в медицине, социологии и
т.п.

\section{Дисперсия суммы}

Пусть измеряемая величина $z=x+y$ складывается из двух \emph{независимы}х
случайных слагаемых $x$ и $y$, для которых известны средние значения
$\average{x}$ и $\average{y}$, и их среднеквадратичные погрешности
$\sigma_{x}$ и $\sigma_{y}$. Непосредственно из определения (\ref{eq:average})
следует вполне очевидный результат, что среднее суммы равно сумме
средних: 
\[
\average{z}=\average{x}+\average{y}.
\]

Найдём дисперсию $\sigma_{z}^{2}$. В силу независимости имеем
\[
\average{\Delta z^{2}}=\average{\Delta x^{2}}+\average{\Delta y^{2}}+2\average{\Delta x\cdot\Delta y}=\average{\Delta x^{2}}+\average{\Delta y^{2}},
\]
то есть:
\begin{equation}
\boxed{{\sigma_{x+y}=\sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}}}}.\label{eq:sigma_sum}
\end{equation}
Таким образом, при сложении \emph{независимых }величин их погрешности
складываются среднеквадратичным образом.

Подчеркнём, что для справедливости соотношения (\ref{eq:sigma_sum})
величины $x$ и $y$ не обязаны быть нормально распределёнными ---
достаточно существования конечных значений их дисперсий (однако можно
показать, что если $x$ и $y$ распределены нормально, нормальным
будет и распределение их суммы).

\note{
Требование независимости
слагаемых является принципиальным. Например, положим $y=x$. Тогда
$z=2x$. Здесь $y$ и $x$, очевидно, зависят друг от друга. Используя
(\ref{eq:sigma_sum}), находим $\sigma_{2x}=\sqrt{2}\sigma_{x}$,
что, конечно, неверно --- непосредственно из определения
следует, что $\sigma_{2x}=2\sigma_{x}$.
}

Отдельно стоит обсудить математическую структуру формулы (\ref{eq:sigma_sum}).
Если если одна из погрешностей много больше другой, например, $\sigma_{x}\gg\sigma_{y}$,
то меньшей погрешностью можно пренебречь, $\sigma_{x+y}\approx\sigma_{x}$.
С другой стороны, если два источника погрешностей имеют один порядок
$\sigma_{x}\sim\sigma_{y}$, то и $\sigma_{x+y}\sim\sigma_{x}\sim\sigma_{y}$. 

Эти обстоятельства важны при планирования эксперимента: как правило,
величина, измеренная наименее точно, вносит наибольший вклад в погрешность
конечного результата. При этом, пока не устранены наиболее существенные
ошибок, бессмысленно гнаться за повышением точности измерения остальных
величин.

\example{
Пусть $\sigma_{y}=\sigma_{x}/3$,
тогда $\sigma_{z}=\sigma_{x}\sqrt{1+\frac{1}{9}}\approx1{,}05\sigma_{x}$,
то есть при различии двух погрешностей более, чем в 3 раза, поправка
к погрешности составляет менее 5\%, и уже нет особого смысла в учёте
меньшей погрешности: $\sigma_{z}\approx\sigma_{x}$. Это утверждение
касается сложения любых независимых источников погрешностей в эксперименте.
}

\section{Погрешность среднего и отдельного измерений\label{sec:average}}

Выборочное среднее арифметическое значение $\average{x}$, найденное
по результатам $n$ измерений, само является случайной величиной.
Действительно, если поставить серию одинаковых опытов по $n$ измерений,
то в каждом опыте получится своё среднее значение, отличающееся от
предельного среднего $x_{0}$.

Вычислим среднеквадратичную погрешность среднего арифметического $\sigma_{\average{x}}$.
Рассмотрим вспомогательную сумму $n$ слагаемых 
\[
Z=x_{1}+x_{2}+\ldots+x_{n}.
\]
Если $\left\{ x_{i}\right\} $ есть набор \emph{независимых} измерений
\emph{одной и той же} физической величины, то мы можем, применяя результат
(\ref{eq:sigma_sum}) предыдущего параграфа, записать
\[
\sigma_{Z}=\sqrt{\sigma_{x_{1}}^{2}+\sigma_{x_{2}}^{2}+\ldots+\sigma_{x_{n}}^{2}}=\sqrt{n}\sigma_{x},
\]
поскольку под корнем находится $n$ одинаковых слагаемых. Отсюда с
учётом $\average{x}=Z/n$ получаем
\begin{equation}
\boxed{{\sigma_{\average{x}}=\frac{\sigma_{x}}{\sqrt{n}}}}.\label{eq:sigma_avg}
\end{equation}

Таким образом, \emph{погрешность среднего значения $x$ по результатам
$n$ независимых измерений оказывается в $\sqrt{n}$ раз меньше погрешности
отдельного измерения}. Это один из важнейших результатов, позволяющий
уменьшать случайные погрешности эксперимента за счёт многократного
повторения измерений.

Подчеркнём отличия между $\sigma_{x}$ и $\sigma_{\average{x}}$:

величина $\sigma_{x}$ --- \emph{погрешность отдельного
измерения} --- является характеристикой разброса значений
в совокупности измерений $\left\{ x_{i}\right\} $, $i=1..n$. При
нормальном законе распределения примерно 68\% измерений попадают в
интервал $\average{x}\pm\sigma_{x}$;

величина $\sigma_{\average{x}}$ --- \emph{погрешность
среднего} --- характеризует точность, с которой определено
среднее значение измеряемой физической величины $\average{x}$ относительно
предельного (<<истинного>>) среднего $x_{0}$;
при этом с доверительной вероятностью $P=68\%$ искомая величина $x_{0}$
лежит в интервале $\average{x}-\sigma_{\average{x}}<x_{0}<\average{x}+\sigma_{\average{x}}$.



% \section{Погрешность погрешности}
% \todo[inline, author = Nozik]{Вот это материал повышенной сложности. Не уверен, что он вообще нужен и уж точно не в середине главы}
% \todo[inline,author = ppv]{Сложности тут особой не вижу, но материал действительно побочный. Но полезный -- ведь это обоснование того, как нужно округлять погрешность.}

% С какой точностью можно вычислить величину $\sigma$ по ограниченному
% количеству $n$ измерений? Оценим среднеквадратичное отклонения от
% своего среднего значения для величины $s$, вычисляемой по формуле
% (\ref{eq:sigma_straight}). Её квадрат $s^{2}$, как нетрудно видеть,
% состоит из $n$ примерно одинаковых слагаемых (обозначим их как $\xi_{i}=\left(x_{i}-\average{x}\right)^{2}$):
% \[
% s^{2}=\frac{1}{n-1}\sum_{i}\xi_{i}.
% \]
% Тогда, повторяя рассуждения п. \ref{subsec:average}, приходим к выводу,
% что погрешность вычисления $s^{2}$ пропорциональна корню из числа
% входящих в неё слагаемых (точнее, нужно использовать число \emph{независимых}
% слагаемых, равное $n-1$, как показано выше):
% \[
% \sigma_{s^{2}}\approx\sqrt{n}\cdot\frac{\sigma_{\xi}}{n-1}\approx
% \frac{\sigma_{\xi}}{\sqrt{n-1}}.
% \]
% С учётом того, что $\average{\xi}\approx s^{2}\approx\sigma_{x}^{2}$,
% величину $\sigma_{\xi}=\sqrt{\average{\left(\xi-\average{\xi}\right)^{2}}}$
% можно по порядку величины оценить как $\sigma_{\xi}\sim\average{\xi}\sim\sigma_{x}^{2}$;
% точный расчёт с использованием распределения Гаусса (\ref{eq:normal})
% даёт $\sigma_{\xi}=\sqrt{2}\sigma_{x}\approx\sqrt{2}s.$ Наконец,
% из соотношения $\sigma_{s^{2}}=2s\sigma_{s}$ (см. формулу (\ref{eq:sxy})
% ниже), окончательно получаем
% \begin{equation}
% \sigma_{s}=\frac{s}{\sqrt{2\left(n-1\right)}}.\label{eq:sigma_sigma}
% \end{equation}
% Более подробный и аккуратный вывод можно найти, например в {[}?{]}.

% Главный вывод, который можно сделать на основании результата (\ref{eq:sigma_sigma})
% --- ошибка вычисления стандартного отклонения, как правило,
% довольно велика. Например, при $n=6$ её относительная величина составляет
% $\approx$30\%, и даже при $n=50$ она уменьшается лишь до $10\%$.
% По этой причине величину погрешности имеет смысл \emph{округлять до
% 1\textendash 2 значащих цифр} (см. также п.~\ref{subsec:round}).

\section{Результирующая погрешность опыта}

Пусть для некоторого результата измерения известна оценка его максимальной
систематической погрешности $\Delta_{\text{сист}}$ и случайная среднеквадратичная
погрешность $\sigma_{\text{случ}}$. Какова <<полная>>
погрешность измерения?

Предположим для простоты, что измеряемая величина \emph{в принципе}
может быть определена сколь угодно точно, так что можно говорить о
некотором её <<истинном>> значении $x_{0}$
(иными словами, погрешность результата связана в основном именно с
процессом измерения). Назовём \emph{полной погрешностью} измерения
среднеквадратичное значения отклонения от результата измерения от
<<истинного>> $x_{0}$: 
\[
\sigma_{\text{полн}}^{2}=\average{\left(x-x_{0}\right)^{2}}.
\]
Отклонение $x-x_{0}$ можно представить как сумму постоянной (но,
вообще говоря, неизвестной) систематической составляющей $\delta x_{\text{сист}}=\mathrm{const}$
и случайной добавки: $x-x_{0}=\delta x_{\text{сист}}+\delta x_{\text{случ}}$.
Причём среднее значение случайной составляющей равно нулю: $\average{\delta x_{\text{случ}}}=0$.
В таком случае находим
\begin{equation}
\sigma_{\text{полн}}^{2}=\average{\delta x_{\text{сист}}^{2}}+\average{\delta x_{\text{случ}}^{2}}\le\Delta_{\text{сист}}^{2}+\sigma_{\text{случ}}^{2}.\label{eq:syst_full}
\end{equation}
Таким образом, для получения \emph{максимального} значения полной
погрешности некоторого измерения нужно среднеквадратично сложить максимальную
систематическую и случайную погрешности.

Если измерения проводятся многократно, то согласно (\ref{eq:sigma_avg})
случайная составляющая погрешности может быть уменьшена, а систематическая
составляющая при этом остаётся неизменной:
\[
\sigma_{\text{полн}}^{2}\le\Delta_{\text{сист}}^{2}+\frac{\sigma_{x}^{2}}{n}.
\]

Отсюда следует важное практическое правило (см. также обсуждение погрешности
суммы п. \ref{subsec:sum}): если случайная погрешность измерений
в 2\textendash 3 раза меньше предполагаемой систематической, то \emph{нет
смысла проводить многократные измерения} в попытке уменьшить погрешность
всего эксперимента. В такой ситуации измерения достаточно повторить
3\textendash 4 раза --- чтобы убедиться в повторяемости
результата, исключить промахи и проверить, что случайная ошибка действительно
мала. В противном случае повторение измерений может иметь смысл до
тех пор, пока погрешность среднего $\sigma_{\average{x}}=\frac{\sigma_{x}}{\sqrt{n}}$
не станет существенно меньше систематической.

\note{
Поскольку конкретная
величина систематической погрешности, как правило, не известна, её
можно в некотором смысле рассматривать наравне со случайной ---
предположить, что её величина была определена по некоторому случайному
закону перед началом измерений (например, при изготовлении линейки
на заводе произошло некоторое случайное искажение шкалы). При такой
трактовке формулу (\ref{eq:syst_full}) можно рассматривать просто
как частный случай формулы сложения погрешностей независимых величин
(\ref{eq:sigma_sum}).\par

Подчеркнем, что вероятностный закон, которому подчиняется
систематическая ошибка, зачастую неизвестен. Поэтому неизвестно и
распределение итогового результата. Из этого, в частности, следует,
что мы не может приписать интервалу $x\pm\Delta_{\text{сист}}$ какую-либо
определённую доверительную вероятность --- она равна 0,68
только если систематическая ошибка имеет нормальное распределение.
Можно, конечно, \emph{предположить},
--- и так часто делают --- что, к примеру, ошибки
при изготовлении линеек на заводе имеют гауссов характер. Также часто
предполагают, что систематическая ошибка имеет \emph{равномерное}
распределение (то есть <<истинное>> значение может с равной вероятностью 
принять любое значение в пределах интервала $\pm\Delta_{\text{сист}}$). 
Строго говоря, для этих предположений нет достаточных оснований.\par
}%\footnotesize

\example{
В результате измерения диаметра проволоки микрометрическим винтом,
имеющим цену деления $h=0,01$ мм, получен следующий набор из $n=8$ значений:\par
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
{\footnotesize{}$d$, мм} & {\footnotesize{}0,39} & {\footnotesize{}0,38} & {\footnotesize{}0,39} & {\footnotesize{}0,37} & {\footnotesize{}0,40} & {\footnotesize{}0,39} & {\footnotesize{}0,38} & {\footnotesize{}0,39}\tabularnewline
\hline 
\end{tabular}\par
Вычисляем среднее значение: $\average{d}\approx386{,}3$~мкм.
Среднеквадратичное отклонение вычисляем по формуле (\ref{eq:sigma_straight}):
$\sigma_{d}\approx9{,}2$~мкм. Случайная погрешность среднего согласно
(\ref{eq:sigma_avg}): $\sigma_{\average{d}}=\frac{\sigma_{d}}{\sqrt{8}}\approx3{,}2$
мкм. Все результаты лежат в пределах $\pm2\sigma_{d}$, поэтому нет
причин сомневаться в нормальности распределения. Максимальную погрешность
микрометра оценим как половину цены деления, $\Delta=\frac{h}{2}=5$~мкм.
Результирующая полная погрешность $\sigma\le\sqrt{\Delta^{2}+\frac{\sigma_{d}^{2}}{8}}\approx6{,}0$~мкм.
Видно, что $\sigma\approx\Delta$ и проводить дополнительные измерения
особого смысла нет. Окончательно результат измерений может быть представлен
в виде (см. также \emph{правила округления}
результатов измерений в п.~\ref{subsec:round})
\[
d=386\pm6\;\text{мкм},\qquad\varepsilon_{d}=1{,}5\%.
\]

Заметим, что поскольку случайная погрешность и погрешность
прибора здесь имеют один порядок величины, наблюдаемый случайный разброс
данных может быть связан как с неоднородностью сечения проволоки,
так и с дефектами микрометра (например, с неровностями зажимов, люфтом
винта, сухим трением, деформацией проволоки под действием микрометра
и т.\,п.). Для ответа на вопрос, что именно вызвало разброс, требуются
дополнительные исследования, желательно с использование более точных
приборов.\par
}%\footnotesize

\example{
Измерение скорости
полёта пули было осуществлено с погрешностью $\delta v=\pm1$ м/c.
Результаты измерений для $n=6$ выстрелов представлены в таблице:\par
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
{\footnotesize{}$v$, м/с} & {\footnotesize{}146} & {\footnotesize{}170} & {\footnotesize{}160} & {\footnotesize{}181} & {\footnotesize{}147} & {\footnotesize{}168}\tabularnewline
\hline 
\end{tabular}\par
Усреднённый результат $\average{v}=162{,}0\;\text{м/с}$,
среднеквадратичное отклонение $\sigma_{v}=13{,}8\;\text{м/c}$, случайная
ошибка для средней скорости $\sigma_{\bar{v}}=\sigma_{v}/\sqrt{6}=5{,}6\;\text{м/с}$.
Поскольку разброс экспериментальных данных существенно превышает погрешность
каждого измерения, $\sigma_{v}\gg\delta v$, он почти наверняка связан
с реальным различием скоростей пули в разных выстрелах, а не с ошибками
измерений. В качестве результата эксперимента представляют интерес
как среднее значение скоростей $\average{v}=162\pm6\;\text{м/с}$
($\varepsilon\approx4\%$), так и значение $\sigma_{v}\approx14\;\text{м/с}$,
характеризующее разброс значений скоростей от выстрела к выстрелу.
Малая инструментальная погрешность в принципе позволяет более точно
измерить среднее и дисперсию, и исследовать закон распределения выстрелов
по скоростям более детально --- для этого требуется набрать
б\'{о}льшую статистику по выстрелам.\par
}%\footnotesize

\example{
Измерение скорости
полёта пули было осуществлено с погрешностью $\delta v=10$ м/c. Результаты
измерений для $n=6$ выстрелов представлены в таблице:\par
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
{\footnotesize{}$v$, м/с} & {\footnotesize{}150} & {\footnotesize{}170} & {\footnotesize{}160} & {\footnotesize{}180} & {\footnotesize{}150} & {\footnotesize{}170}\tabularnewline
\hline 
\end{tabular}\par
Усреднённый результат $\average{v}=163{,}3\;\text{м/с}$,
$\sigma_{v}=12{,}1\;\text{м/c}$, $\sigma_{\bar{v}}=5\;\text{м/с}$,
$\sigma_{\text{полн}}\approx11{,}2\;\text{м/с}$. Инструментальная
погрешность каждого измерения превышает разброс данных, поэтому в
этом опыте затруднительно сделать вывод о различии скоростей от выстрела
к выстрелу. Результат измерений скорости пули: $\average{v}=163\pm11\;\text{м/с}$,
$\varepsilon\approx7\%$. Проводить дополнительные выстрелы при такой
большой инструментальной погрешности особого смысла нет ---
лучше поработать над точностью приборов и методикой измерений.\par
}%\footnotesize

\section{Обработка косвенных измерений\label{sec:kosv}}

\emph{Косвенными} называют измерения, полученные в результате расчётов,
использующих результаты \emph{прямых} (то есть <<непосредственных>>)
измерений физических величин. Сформулируем основные правила пересчёта
погрешностей при косвенных измерениях.

\subsection{Случай одной переменной}

Пусть в эксперименте измеряется величина $x$, а её <<наилучшее>>
(в некотором смысле) значение равно $x^{\star}$ и оно известно с
погрешностью $\sigma_{x}$. После чего с помощью известной функции
вычисляется величина $y=y\!\left(x\right)$.

В качестве <<наилучшего>> приближения для
$y$ используем значение функции при <<наилучшем>>
$x$:
\[
y^{\star}=y\!\left(x^{\star}\right).
\]

Найдём величину погрешности $\sigma_{y}$. Обозначая отклонение измеряемой
величины как $\Delta x=x-x^{\star}$, и пользуясь определением производной,
при условии, что функция $y\left(x\right)$ --- гладкая
вблизи $x\approx x^{\star}$, запишем 
\[
\Delta y\equiv y\left(x\right)-y\left(x^{\star}\right)\approx y'\cdot\Delta x,
\]
где $y'\equiv\frac{dy}{dx}$ --- производная, взятая в точке
$x^{\star}$. Возведём полученное в квадрат, проведём усреднение ($\sigma_{y}^{2}=\average{\Delta y^{2}}$,
$\sigma_{x}^{2}=\average{\Delta x^{2}}$), и затем снова извлечём
корень. В результате получим
\begin{equation}
\boxed{{\sigma_{y}=\left|\frac{dy}{dx}\right|\sigma_{x}.}}\label{eq:sxy}
\end{equation}

\example{
Для степенной функции
$y=Ax^{n}$ имеем $\sigma_{y}=nAx^{n-1}\sigma_{x}$, откуда 
\[
\frac{\sigma_{y}}{y}=n\frac{\sigma_{x}}{x},\qquad\text{или}\qquad\varepsilon_{y}=n\varepsilon_{x},
\]
то есть относительная погрешность степенной функции возрастает пропорционально
показателю степени $n$.\par
}%\footnotesize

\example{
$\varepsilon_{1/x}=\varepsilon_{x}$
--- при обращении величины сохраняется её относительная
погрешность.\par
}%\footnotesize

\note{ 
\textbf{Упражнение.} Найдите погрешность
логарифма $y=\ln x$, если известны~$x$ и~$\sigma_{x}$.\par
}%\footnotesize

\note{ 
\textbf{Упражнение.} Найдите погрешность
показательной функции $y=a^{x}$, если известны~$x$ и~$\sigma_{x}$.
Коэффициент $a$ задан точно.\par
}%\footnotesize

\subsection{Случай многих переменных}

Пусть величина $u$ вычисляется по измеренным значениям нескольких
различных \emph{независимых} физических величин $x$, $y$, $\ldots$
на основе известного закона $u=f\!\left(x,y,\ldots\right)$. В качестве
наилучшего значения можно по-прежнему взять значение функции $f$
при наилучших значениях измеряемых параметров:
\[
u^{\star}=f\!\left(x^{\star},y^{\star},\ldots\right).
\]

Для нахождения погрешности $\sigma_{u}$ воспользуемся свойством,
известным из математического анализа, --- малые приращения
функции многих переменных складываются линейно, то есть справедлив
\emph{принцип суперпозиции} малых приращений:
\[
\Delta u\approx f'_{x}\cdot\Delta x+f'_{y}\cdot\Delta y+\ldots,
\]
где символом $f'_{x}\equiv\frac{\partial f}{\partial x}$ обозначена
\emph{частная производная} функции $f$ по переменной $x$ ---
то есть обычная производная $f$ по $x$, взятая при условии, что
все остальные аргументы (кроме $x$) считаются постоянными параметрами.
Тогда пользуясь формулой для нахождения дисперсии суммы независимых
величин (\ref{eq:sigma_sum}), получим соотношение, позволяющее вычислять
погрешности косвенных измерений для произвольной функции $u=f\left(x,y,\ldots\right)$:
\begin{equation}
\boxed{\sigma_{u}^{2}=f_{x}^{\prime2}\,\sigma_{x}^{2}+f_{y}^{\prime2}\,\sigma_{y}^{2}+\ldots}\label{eq:sigma_general}
\end{equation}
Это и есть искомая общая формула пересчёта погрешностей при косвенных
измерениях.

Отметим, что формулы (\ref{eq:sxy}) и (\ref{eq:sigma_general}) применимы
только если относительные отклонения всех величин малы ($\varepsilon_{x},\varepsilon_{y},\ldots\ll1$),
а измерения проводятся вдали от особых точек функции $f$ (производные
$f_{x}'$, $f_{y}'$ $\ldots$ не должны обращаться в бесконечность).
Также подчеркнём, что все полученные здесь формулы справедливы только
для \emph{независимых} переменных $x$, $y$, $\ldots$

Остановимся на некоторых важных частных случаях формулы (\ref{eq:sigma_general}).

\paragraph{Погрешность суммы.}

Для суммы (или разности) $u=\sum\limits _{i=1}^{n}a_{i}x_{i}$ имеем
\begin{equation}
\sigma_{u}^{2}=\sum_{i=1}^{n}a_{i}^{2}\sigma_{x_{i}}^{2}.
\end{equation}


\paragraph{Погрешность степенной функции.}

Пусть $u=x^{\alpha}\cdot y^{\beta}\cdot\ldots$. Тогда нетрудно получить,
что
\[
\frac{\sigma_{u}^{2}}{u^{2}}=\alpha^{2}\frac{\sigma_{x}^{2}}{x^{2}}+\beta^{2}\frac{\sigma_{y}^{2}}{y^{2}}+\ldots
\]
или через относительные погрешности
\begin{equation}
\varepsilon_{u}^{2}=\alpha^{2}\varepsilon_{x}^{2}+\beta^{2}\varepsilon_{y}^{2}+\ldots\label{eq:espilon_power}
\end{equation}


\paragraph{Погрешность произведения и частного.}

Пусть $u=xy$ или $u=x/y$. Тогда в обоих случаях имеем
\begin{equation}
\varepsilon_{u}^{2}=\varepsilon_{x}^{2}+\varepsilon_{y}^{2},
\end{equation}
то есть при умножении или делении относительные погрешности складываются
квадратично.

{\footnotesize
\textbf{Пример.} Рассмотрим несколько
более сложный пример: нахождение угла по его тангенсу 
\[
u=\arctg\frac{y}{x}.
\]
В таком случае, пользуясь тем, что $\left(\arctg z\right)'=\frac{1}{1+z^{2}}$,
где $z=y/x$, и используя производную сложной функции, находим $u_{x}'=u_{z}'z'_{x}=-\frac{y}{x^{2}+y^{2}}$,
$u_{y}'=u'_{z}z'_{y}=\frac{x}{x^{2}+y^{2}}$, и наконец 
\[
\sigma_{u}^{2}=\frac{y^{2}\sigma_{x}^{2}+x^{2}\sigma_{y}^{2}}{\left(x^{2}+y^{2}\right)^{2}}.
\]
}%\footnotesize

{\footnotesize
\textbf{Упражнение.} Найти погрешность
вычисления гипотенузы $z=\sqrt{x^{2}+y^{2}}$ прямоугольного треугольника
по измеренным катетам $x$ и $y$.\par
}%\footnotesize

По итогам данного раздела можно дать следующие практические рекомендации.
\begin{itemize}
\item Как правило, нет смысла увеличивать точность измерения какой-то одной
величины, если другие величины, используемые в расчётах, остаются
измеренными относительно грубо --- всё равно итоговая погрешность
скорее всего будет определяться самым неточным измерением. Поэтому
все измерения имеет смысл проводить \emph{примерно с одной и той же
относительной погрешностью}. 
\item При этом, как следует из (\ref{eq:espilon_power}), особое внимание
следует уделять измерению величин, возводимых при расчётах в степени
с большими показателями. А при сложных функциональных зависимостях
имеет смысл детально проанализировать структуру формулы (\ref{eq:sigma_general}):
если вклад от некоторой величины в общую погрешность мал, нет смысла
гнаться за высокой точностью её измерения, и наоборот, точность некоторых
измерений может оказаться критически важной.
\item Следует избегать измерения малых величин как разности двух близких
значений (например, толщины стенки цилиндра как разности внутреннего
и внешнего радиусов): если $u=x-y$, то абсолютная погрешность $\sigma_{u}=\sqrt{\sigma_{x}^{2}+\sigma_{y}^{2}}$
меняется мало, однако относительная погрешность $\varepsilon_{u}=\frac{\sigma_{u}}{x-y}$
может оказаться неприемлемо большой, если $x\approx y$.
\end{itemize}