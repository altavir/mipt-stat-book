\chapter{Оценка параметров}
\label{ch:estimate}

% \disclaimer{Эта глава рекомендуется к прочтению студентам,
%     прослушавшим базовый курс теории вероятностей.
%     Студентам первого курса рекомендуется ограничиться ознакомлением
%     с методом наименьших квадратов (п.~\ref{sec:MNK}).}

%  В этой главе представлена только очень краткая выжимка из теории оценок. Для
% должного понимания этой главы математической статистики следует обратить
% внимание на дополнительные материалы
% \todo{ссылка}.
% }

% \disclaimer{
%     В этой главе представлена только очень краткая выжимка из теории оценок. Для
% должного понимания этой главы математической статистики следует обратить
% внимание на дополнительные материалы
% \todo{ссылка}.
% }

% \section{Оценка параметров}

% Процедуру определения физических параметров по измерениями называют оцениванием,
% а результат этой процедуры~--- оценкой. Такая терминология связана с тем, что
% никакое физическое значение не может быть измерено с абсолютной точностью.
% Всегда есть некоторая погрешность измерения. В простых случаях, оценки могут
% быть получены простым усреднением результатов измерений или значений каких-то
% функций этих результатов. Но в большинстве случаев измеряется не одна величина,
% а зависимость одной величины от другой. В этом случае требуется построить оценку
% параметра этой зависимости.

Цель любого физического эксперимента~--- проверить, выполняется ли некоторая
теоретическая закономерность (\emph{модель}), а также получить или уточнить
её параметры. Поскольку набор экспериментальных данных неизбежно ограничен,
а каждое отдельное измерение имеет погрешность, можно говорить лишь
об \emph{оценке} этих параметров. В большинстве случаев измеряется не одна
величина, а некоторая функциональная зависимость величин друг от друга.
В таком случае возникает необходимость построить оценку параметров этой зависимости.

\example{Рассмотрим процедуру измерения сопротивления некоторого резистора.
Простейшая теоретическая модель для резистора~--- закон Ома $U=RI$,
где сопротивление $R$~--- единственный параметр модели. Часто при измерениях
возможно возникновение систематической ошибки~--- смещение нуля напряжения или тока.
Тогда для получения более корректной оценки сопротивления стоит использовать
модель с двумя параметрами: $U=RI+U_0$.}

% \example{
%     Пусть есть резистор, подключенный к источнику постоянного напряжения
% (напряжение можно менять), амперметр и вольтметр. Нужно определить сопротивление
% $R$ этого резистора. Для того, чтобы получить значение можно провести ряд
% измерений с одним или разными напряжениями $U$, посчитать в каждом случае
% значение сопротивления как отношение напряжения к току: $R = U / I$, и усреднить
% результаты. Такой метод можно использовать, но при этом результат базируется на
% предположении о том, что зависимость напряжения от тока является строго
% линейной, что, вообще говоря, соблюдается не всегда (к примеру может
% существовать дополнительное паразитное сопротивление).
%
%     Более корректным способом является измерение тока для различных значений
% напряжения, построения линейной зависимости и оценки ее параметров. В этом
% случае точность оценки остается такой же, но при этом возникает возможность
% визуальной или аналитической проверки соответствия данных зависимости.
% %     \todo[inline]{добавить картику?}
% }

Для построения оценки нужны следующие компоненты
\begin{itemize}
    \item \emph{данные}~--- результаты измерений $\{x_i, y_i\}$
    и их погрешности $\{\sigma_i\}$
    (экспериментальная погрешность является неотъемлемой
    частью набора данных!);
    \item \emph{модель} $y=f(x; {\theta_1,\theta_2,\ldots })$~---
параметрическое описание исследуемой зависимости
($\theta$~--- набор параметров модели, например,
коэффициенты $\{k,\,b\}$ прямой $f(x)=kx+b$);
\item процедура построения оценки параметров по
измеренным данным (<<оценщик>>):
%% лишняя терминология?
\[
\theta \approx \hat{\theta}(\{x_i,\,y_i,\,\sigma_i\}).
\]
\end{itemize}


\section{Метод минимума хи-квадрат}
\label{sec:chi2}

Обозначим отклонения результатов некоторой серии измерений от теоретической
модели $y=f(x; \theta)$ как
\[
 \Delta y_i = y_i- f(x_i; \theta),\qquad i= 1\ldots n,
\]
где $\theta$~--- некоторый параметр (или набор параметров),
для которого требуется построить наилучшую оценку. Нормируем $\Delta y_i$
на стандартные отклонения $\sigma_i$ и построим сумму
\begin{equation}
    \chi^2 = \sum_i{\left(\frac{\Delta y_i}{\sigma_i}\right)^2},
    \label{eq:chi2}
\end{equation}
которую принято называть суммой \emph{хи-квадрат}.

Метод \emph{минимума хи-квадрат} (\emph{метод Пирсона}) заключается в подборе такого
$\theta$, при котором сумма квадратов отклонений от теоретической
модели, нормированных на ошибки измерений, достигает минимума:
\[
\chi^2(\theta) \to \mathrm{min}.
\]

\note{Подразумевается, что погрешность измерений $\sigma_i$ указана только для
вертикальной оси $y$. Поэтому, при использовании метода следует выбирать оcи
таким образом, чтобы относительная ошибка по оси абсцисс была значительно меньше,
чем по оси ординат.}
% % а если у осей разные размерности???
% Если ошибки сопоставимы, то в качестве $\sigma_i$ можно брать среднеквадратичную ошибку по
% обеим осям: $\sigma_i^2 = \sigma_{x,i}^2 + \sigma_{y,i}^2$.}

Данный метод вполне соответствует нашему интуитивному представлению
о том, как теоретическая зависимость должна проходить через экспериментальные
точки. Ясно, что чем ближе данные к модельной кривой, тем
меньше будет сумма $\chi^2$. При этом, чем больше погрешность точки, тем
в большей степени дозволено результатам измерений отклоняться от модели.
Метода минимума $\chi^2$ является частным случаем
более общего \emph{метода максимума правдоподобия} (см. ниже),
реализующийся при \emph{нормальном} (\emph{гауссовом}) распределении ошибок.

% так ли это?
Можно показать (см. []), что оценка по методу хи-квадрат является состоятельной,
несмещенной и, если данные распределены нормально,
имеет максимальную эффективность.

% \note{Оценка методом минимума $\chi^2$ может быть использована в подавляющем
% большинстве случаев, но все-таки не является универсальной. В случае с особо
% точными измерениями рекомендуется обратиться к дополнительной литературе.}

\note{Простые аналитические выражения для оценки методом хи-квадрат существуют
    (см. п. \ref{sec:MNK}, \ref{sec:chi2lin}) только в случае линейной
    зависимости $f(x)=kx+b$ (впрочем, нелинейную зависимость часто можно
    заменой переменных свести к линейной). В общем случае задача поиска
    минимума $\chi^2(\theta)$ решается численно, а соответствующая процедура
    реализована в большинстве специализированных программных пакетов
    по обработке данных.}


\paragraph{Метод максимального правдоподобия.} Рассмотрим кратко один
из наиболее общих методов оценки параметров зависимостей~---
метод максимума правдоподобия.

Сделаем два ключевых предположения:
\begin{itemize}
 \item  зависимость между измеряемыми величинами действительно может
 быть описана функцией $y=f(x;\theta)$ при некотором $\theta$;
 \item все отклонения $\Delta y_i$ результатов измерений от теоретической модели
 являются \emph{независимыми} и имеют \emph{случайный} (не систематический!) характер.
\end{itemize}

Пусть $P(\Delta y_i)$~--- вероятность обнаружить отклонение $\Delta y_i$
при фиксированных $\{x_i\}$, погрешностях $\{\sigma_i\}$ и параметрах модели $\theta$.
Построим функцию, равную вероятности обнаружить
весь набор отклонений $\{\Delta y_1,\ldots,\Delta y_n\}$. Ввиду независимости
измерений она равна произведению вероятностей:
\begin{equation}\label{eq:L}
L =  \prod_{i=1}^n P(\Delta y_i).
\end{equation}
Функцию $L$ называют \emph{функцией правдоподобия}.

Метод максимума правдоподобия заключается в поиске такого $\theta$,
при котором наблюдаемое отклонение от модели будет иметь
\emph{наибольшую вероятность}, то есть
\[
L(\theta) \to \mathrm{max}.
\]
\note{Поскольку с суммой работать удобнее, чем с произведениями, чаще
    используют не саму функцию $L$, а её логарифм:
    \[
    \ln L = \sum_i \ln P(\Delta y_i).
    \]%
}

Пусть теперь ошибки измерений имеют \emph{нормальное} распределение
(напомним, что согласно центральной предельной теореме нормальное распределение
применимо, если отклонения возникают из-за большого
числа независимых факторов, что на практике реализуется довольно часто).
Согласно \eqref{eq:normal}, вероятность обнаружить в $i$-м измерении
отклонение $\Delta y_i$ пропорциональна величине
\[
P(\Delta y_i) \propto e^{-\frac{\Delta y_i^2}{2\sigma_i^2}},
\]
где $\sigma_i$~--- стандартная ошибка измерения величины $y_i$. Тогда
логарифм функции правдоподобия \eqref{eq:L} будет равен (с точностью до константы)
\[
 \ln L = - \sum_i \frac{\Delta y_i^2}{2\sigma_i^2} = - \frac12 \chi^2.
\]
Таким образом, максимум правдоподобия действительно будет соответствовать
минимуму $\chi^2$.

\paragraph{Метод наименьших квадратов (МНК).}

Рассмотрим случай, когда все погрешности измерений одинаковы,
$\sigma_i=\mathrm{const}$. Тогда множитель $1/\sigma^2$ в сумме $\chi^2$
выносится за скобки, и оценка параметра сводится к нахождению минимума суммы
квадратов отклонений:
\begin{equation}
S(\theta) = \sum_{i=1}^n \left(y_i - f(x_i;\theta)\right)^2 \to \mathrm{min}.
\end{equation}

% Эта оценка очевидно сохраняет все свойства оценки
% минимума $\chi^2$, правда только в том случае, если все ошибки
% действительно одинаковы.
% Она является состоятельной и асимптотически
% несмещенной, хотя ее эффективность оптимальна только для ограниченного
% количества случаев. В некоторых случаях, ошибку измерения можно оценить
% по разбросу данных, используя критерий Пирсона.
%% это как?

Оценка по \emph{методу наименьших квадратов} (МНК) удобна в том случае,
когда не известны погрешности отдельных измерений. Однако тот  факт, что
метод МНК игнорирует информацию о погрешностях, является и его основным
недостатком. В частности, это не позволяет определить точность оценки
(например, погрешности коэффициентов прямой $\sigma_k$ и
$\sigma_b$) без привлечения дополнительных предположений
(см. п.~\ref{sec:MNKerror} и~\ref{sec:MNKdefect}).

% \paragraph{Интервальная оценка}

% Оценка методом наименьших квадратов очевидно игнорирует информацию о
% погрешностях измерений и не позволяет напрямую оценить погрешность
% результата без дополнительных предположений.
% Для получения оценки
% погрешностей надо сделать три предположения:
% %% предположения уже сделаны выше
% \begin{itemize}
%     \item  Данные описываются предложенной моделью;
%     \item  Отклонения данных от модельной кривой независимы между собой (носят
%   статистический характер);
%     \item  Статистические ошибки для всех точек равны между собой.
% \end{itemize}

% Оценить ошибку результатов оценки по методу наименьших квадратов
% (например, погрешности коэффициентов прямой $\sigma_k$ и $\sigma_b$
% можно следующим образом. В рамках сделанных предположениях
% (ошибки \emph{независимы} и \emph{одинаковы})
% величину $\sigma$ можно оценить для каждой из точек
% как средне квадратичное отклонение точек от наилучшей модели (той,
% которая получена минимизацией суммы $Q$). После этого задача получения
% погрешностей сводится к уже решенной для метода $\chi^2$.
%


\section{Проверка гипотез методом хи-квадрат}

\subsection{Проверка качества аппроксимации}

Значение суммы $\chi^2$ позволяет оценить, насколько хорошо данные описываются
предлагаемой моделью $y=f(x;\theta)$.

Предположим, что распределение ошибок при измерениях \emph{нормальное}.
Тогда можно ожидать, что большая часть отклонений данных от модели будет
порядка одной среднеквадратичной ошибки: $\Delta y_{i}\sim\sigma_{i}$.
Следовательно, сумма хи-квадрат \eqref{eq:chi2} окажется по порядку
величины равна числу входящих в неё слагаемых: $\chi^{2}\sim n$.

\note{Точнее, если функция $f\!\left(x;\theta_1,\,\ldots,\, \theta_p\right)$
содержит~$p$ подгоночных параметров
(например, $p=2$ для линейной зависимости $f\!\left(x\right)=kx+b$),
то при заданных $\theta$ лишь $n-p$ слагаемых в сумме хи-квадрат будут независимы.
Иными словами, когда параметры $\theta$ определены
из условия минимума хи-квадрат, сумму $\chi^{2}$ можно рассматривать как функцию
$n-p$ переменных. Величину $n-p$ называют \emph{числом степеней свободы} задачи.}

В теории вероятностей доказывается (см. \cite{hudson} или \cite{idie}),
что ожидаемое среднее значение (математическое ожидание) суммы $\chi^{2}$
в точности равно числу степеней свободы:
\[
\limaverage{\chi^{2}}=n-p.
\]

Таким образом, при хорошем соответствии модели и данных,
величина $\chi^2 / (n-p) $ должна в среднем быть равна единице.
Значения существенно большие (2 и выше) свидетельствуют либо о
\emph{плохом соответствии теории и результатов измерений},
    либо о \emph{заниженных погрешностях}.
    Значения меньше 0,5 как правило свидетельствуют о \emph{завышенных погрешностях}.

\note{Чтобы дать строгий количественный критерий, с какой долей вероятности
    гипотезу $y=f\!\left(x\right)$ можно считать подтверждённой или опровергнутой,
    нужно знать вероятностный закон, которому подчиняется функция $\chi^{2}$.
    Если ошибки измерений распределены нормально, величина хи-квадрат подчинятся
    одноимённому распределению (с $n-p$ степенями свободы).
    В элементарных функциях \emph{распределение хи-квадрат} не выражается,
    но может быть легко найдено численно: функция встроена во все основные
    статистические пакеты, либо может быть вычислена по таблицам.}

\subsection{Оценка погрешности параметров}

Важным свойством метода хи-квадрат является <<встроенная>> возможность
нахождения погрешности вычисленных параметров $\sigma_{\theta}$.
%% не понятно! почему парабола?
% Для этого можно использовать тот факт, что сама зависимость $\chi^2(\theta)$
% в большинстве случае имеет вид параболы
% , а величина $\exp{- \chi^2(\theta)/2}$
% пропорциональна так называемой функции правдоподобия,
% то есть вероятности получить заданный набор данных при
% фиксированной модели и параметрах $\theta$.
Пусть функция $\chi^2(\theta)$ имеет максимум при
$\theta = \limaverage{\theta}$. Тогда в окрестности $\limaverage{\theta}$ она
имеет вид параболы
\[
\chi^2(\theta) \propto (\theta - \limaverage{\theta})^2
\]

Величина $\exp{- \chi^2(\theta)/2}$ пропорциональна функции правдоподобия $L$,
то есть вероятности получить заданный набор данных при фиксированной
модели и параметрах $\theta$. Легко видеть, что функция правдоподобия
в этом случае имеет вид нормального распределения и для
% а какая у этого распределения дисперсия??
определения вероятностного содержания различных интервалов можно пользоваться
свойствами этого распределения. Для практической оценки достаточно найти такие
значения $\theta$, при которых значение $\chi^2$ будет отличаться от
минимального на 1. Эти значения будут соответствовать краям интервала с
вероятностным содержанием 68\% или $1-\sigma$ интервалу. Отклонение на 2 будет
соответствовать уже 95\% доверительному интервалу.
% почему?




% требуется доработка!

% \section{Многопараметрические оценки}
%
%     Однопараметрические оценки очень просты для понимания и реализации, но
% довольно редко встречаются на практике. Даже при оценке параметров
% линейно зависимости вида $y = k x + b$ уже существует два параметра:
% $k$~--- наклон прямой и $b$~--- смещение. Все перечисленные выше
% математические методы отлично работают и в многомерном случае, но
% процесс поиска экстремума функции (максимума в случае метода максимума
% правдоподобия и минимума в случае методов семейства наименьших
% квадратов) и интерпретация результатов требуют использования специальных
% программных пакетов.
%
%
% \subsection{Доверительные области в многомерном случае}
%
%     Принцип построения доверительной области в многомерном случае точно
% такой же, как и для одномерных доверительных интервалов. Требуется найти
% такую областью пространства параметров $\Omega$, для которой
% вероятностное содержание для оценки параметра $\hat \theta$ (или
% самого параметра $\theta$ в засимости от того, какой философии вы
% придерживаетесь) будет равно некоторой наперед заданной величине
% $\alpha$:
% \begin{equation}
% P(\theta \in \Omega) = \int_\Omega{L(X | \theta)}d\Omega = \alpha.
% \end{equation}
%
% Реализация на практике этого определения сталкивается с тремя
% проблемами:
%
% \begin{enumerate}
% \item Взятие многомерного интеграла от произвольной функции~--- не тривиальная
%   задача. Даже в случае двух параметров, уже требуется некоторый уровень
%   владения вычислительной математикой и компьютерными методами. В случае
%   большего числа параметров, как правило надо использовать специально
%   разработанные для этого пакеты.
% \item Определить центральный интервал для гипер-области гораздо сложнее, чем
%   сделать это для одномерного отрезка. Единых правил для выбора такой
%   области не существует.
% \item Даже если удалось получить доверительную область, описать такой объект
%   в общем случае не так просто, так что представление результатов
%   составляет определенную сложность.
% \end{enumerate}
%
% Для решения этих проблем, пользуются следующим приемом: согласно
% центральной предельной теореме, усреднение большого количества одинаково
% распределенных величин дает нормально распределенную величину. Это же
% верно и в многомерном случае. В большинстве случаев, мы ожидаем, что
% функция правдоподобия будет похожа на многомерное нормальное
% распределение:
% \begin{equation}
%     L(\theta) = \frac{1}{(2 \pi)^{n/2}\left|\Sigma\right|^{1/2}} e^{-\frac{1}{2}
% (x - \mu)^T \Sigma^{-1} (x - \mu)},
% \end{equation}
% где n~--- размерность вектора параметров, $\mu$~--- вектор
% наиболее вероятных значений, а $\Sigma$~---
% %
% % \href{
% % https://ru.wikipedia.org/wiki/\%D0\%9A\%D0\%BE\%D0\%B2\%D0\%B0\%D1\%80\%D0\%B8\%
% % D0\%B0\%D1\%86\%D0\%B8\%D0\%BE\%D0\%BD\%D0\%BD\%D0\%B0\%D1\%8F_\%D0\%BC\%D0\%B0\
% %D1\%82\%D1\%80\%D0\%B8\%D1\%86\%D0\%B0}
% % {
% ковариационная матрица
% % }
% распределения.
%
% Для многомерного нормального распределения, линии постоянного уровня (то
% есть поверхности, на которых значение плотности вероятности одинаковые)
% имеют вид гипер-эллипса, определяемого уравнением
% $(x~--- \mu)^T \Sigma^{-1} (x~--- \mu) = const$. Для любого вероятностного
% содержания $\alpha$ можно подобрать эллипс, который будет
% удовлетворять условию на вероятностное содержание. Интерес правда
% редставляет не эллипс (в случае размерности больше двух, его просто
% невозможно отобразить), а ковариацонная матрица. Диагональные элементы
% этой матрицы являются дисперсиями соответствующих параметров (с учетом
% всех корреляций параметров).

\section{Методы построения наилучшей прямой}
Применим перечисленные выше методы к задаче о построении наилучшей прямой
$y=kx+b$ по экспериментальным точкам $\{x_i,\,y_i\}$.
Линейность функции позволяет записать решение в относительно
простом аналитическом виде.

Обозначим расстояние от $i$-й экспериментальной точки до искомой прямой,
измеренное по вертикали, как
\[
\Delta y_{i}=y_{i}-\left(kx_{i}+b\right),
\]
и найдём такие параметры $\{k,b\}$, чтобы <<совокупное>> отклонение
результатов от линейной зависимости было в некотором смысле минимально.

\subsection{Метод наименьших квадратов}\label{sec:MNK}
\label{sec:linear}
Пусть сумма квадратов расстояний от точек до прямой минимальна:
\begin{equation}\label{eq:mnk_S}
S\!\left(k,b\right)=
\sum\limits _{i=1}^{n}(y_i-(kx_i+b))^{2}\to\mathrm{min}.
\end{equation}

Рассмотрим сперва более простой частный случай, когда искомая прямая
заведомо через <<ноль>>, то есть $b=0$ и $y=kx$.
Необходимое условие минимума функции $S\left(k\right)$, как известно,
есть равенство нулю её производной. Дифференцируя сумму (\ref{eq:mnk_S})
по $k$, считая все величины $\left\{ x_{i},\,y_{i}\right\} $ константами,
найдём
\[
\frac{dS}{dk}=-\sum\limits _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}\right)=0.
\]
Решая относительно $k$, находим
\[
k=\frac{\sum\limits _{i=1}^{n}x_{i}y_{i}}{\sum\limits _{i=1}^{n}x_{i}^{2}}.
\]
Поделив числитель и знаменатель на $n$, этот результат можно записать
более компактно:
\begin{equation}
k=\frac{\left\langle xy\right\rangle }{\left\langle x^{2}\right\rangle}.
\label{eq:MNK0}
\end{equation}
Напомним, что угловые скобки означают усреднение по всем экспериментальным точкам:
\[
\left\langle \ldots\right\rangle \equiv\frac{1}{n}\sum\limits
_{i=1}^{n}\left(\ldots\right)_{i}
\]

В общем случае при $b\ne0$ функция $S\left(k,b\right)$ должна иметь
минимум как по $k$, так и по $b$. Поэтому имеем систему из двух
уравнений $\partial S/\partial k=0$, $\partial S/\partial b=0$,
% \begin{align*}
% \frac{\partial S}{\partial k} & =-\sum\limits
% _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}-b\right)=0,\\
% \frac{\partial S}{\partial b} & =-\sum\limits
% _{i=1}^{n}2\left(y_{i}-kx_{i}-b\right)=0.
% \end{align*}
решая которую, можно получить (получите самостоятельно):
\begin{equation}
    k=\frac{\average{xy}-\average{x}\average{y}}%
        {\average{x^{2}}-\average{x}^2},\qquad
        b=\average{y}-k\average{x}.\label{eq:MNK}
\end{equation}
Эти соотношения и есть решение задачи о построении наилучшей прямой
методом наименьших квадратов.

\note{Совсем кратко формулу (\ref{eq:MNK}) можно записать, если ввести обозначение
    \begin{equation}
        D_{xy}\equiv \average{xy} - \average{x}\average{y} =
        \average{x-\average{x}}\cdot
        \average{y-\average{y}}.
\label{eq:cov}
    \end{equation}
В математической статистике величину $D_{xy}$ называют \emph{ковариацией}.
При $x\equiv y$ имеем дисперсию
$D_{xx}=\average{(x-\average{x})^2}$.
    Тогда
    \begin{equation}
        k=\frac{D_{xy}}{D_{xx}},\qquad b=\left\langle y\right\rangle
-k\left\langle x\right\rangle .\label{eq:MNK_short}
    \end{equation}
}

\subsection{Погрешность МНК в линейной модели}\label{sec:MNKerror}

Погрешности $\sigma_{k}$ и $\sigma_{b}$ коэффициентов, вычисленных
по формуле (\ref{eq:MNK}) (или (\ref{eq:MNK0})), можно оценить в
следующих предположениях.
Пусть погрешность измерений величины $x$ пренебрежимо мала: $\sigma_{x}\approx0$,
а погрешности по $y$ \emph{одинаковы} для всех экспериментальных точек
$\sigma_{y}=\mathrm{const}$, \emph{независимы} и имеют \emph{случайный} характер
(систематическая погрешность отсутствует).

Пользуясь в этих предположениях формулами для погрешностей косвенных
измерений (см. раздел (\ref{sec:kosv})) можно получить следующие
соотношения (выкладки здесь весьма громоздки, подробности можно найти
в п. (\ref{subsec:MMP})):
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^{2}\right)},
\label{eq:MNK_sigma_k}
\end{equation}
\begin{equation}
\sigma_{b}=\sigma_{k}\sqrt{\left\langle x^{2}\right\rangle
},\label{eq:MNK_sigma_b}
\end{equation}
где использованы введённые выше сокращённые обозначения (\ref{eq:cov}).
Коэффициент $n-2$ отражает число независимых <<степеней
свободы>>: $n$ экспериментальных точек за вычетом двух
условий связи (\ref{eq:MNK}).

В частном случае $y=kx$:
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-1}\left(\frac{\left\langle y^{2}\right\rangle
}{\left\langle x^{2}\right\rangle }-k^{2}\right)}.\label{eq:MNK_sigma0}
\end{equation}


\subsection{Недостатки и условия применимости МНК}\label{sec:MNKdefect}

Формулы (\ref{eq:MNK}) (или (\ref{eq:MNK0})) позволяют провести
прямую по \emph{любому} набору экспериментальных данных, а полученные
выше соотношения~--- вычислить
соответствующую среднеквадратичную ошибку для её коэффициентов. Однако
далеко не всегда результат будет иметь физический смысл. Перечислим
ограничения применимости данного метода.

% По очевидным причинам оценка погрешностей, проведенная таким образом, не имеет
% смысла для маленького количества измерений (меньше 8-10 точек). Все будет
% работать и будет получен какой-то результат, но он будет довольно бессмысленным.

В первую очередь метод наименьших квадратов~--- статистический,
и поэтому он предполагает использование достаточно большого количества
экспериментальных точек (желательно $n>10$).

Поскольку метод предполагает наличие погрешностей только по $y$,
оси следует выбирать так, чтобы погрешность $\sigma_{x}$ откладываемой
по оси абсцисс величины была минимальна.

Кроме того, метод предполагает, что все погрешности в опыте~---
случайны. Соответственно, формулы (\ref{eq:MNK_sigma_k})--(\ref{eq:MNK_sigma0})
применимы \emph{только для оценки случайной составляющей} ошибки $k$
или $b$. Если в опыте предполагаются достаточно большие систематические
ошибки, они должны быть оценены \emph{отдельно}. Отметим, что для
оценки систематических ошибок не существует строгих математических
методов, поэтому в таком случае проще и разумнее всего воспользоваться
графическим методом.

Одна из основных проблем, связанных с определением погрешностей методом
наименьших квадратов заключается в том, что он дает разумные погрешности даже в
том случае, когда данные вообще не соответствуют модели.
Если погрешности измерений известны, предпочтительно использовать
метод минимума~$\chi^2$.
% Если других инструментов под рукой нет, то  результаты работы
% метода надо всегда проверять визуально по графику.

Наконец, стоит предостеречь от использования \emph{любых} аналитических
методов <<вслепую>>, без построения графиков. В частности, МНК не способен
выявить такие <<аномалии>>, как отклонения от линейной зависимости,
немонотонность, случайные всплески и т.п. Все эти случаи требуют особого
рассмотрения и могут быть легко обнаружены визуально при построении графика.



% Резюмируя, можно сформулировать универсальную практическую рекомендацию:
% если результаты какого-либо математического метода обработки данных
% существенно расходятся с тем, что можно получить <<вручную>>
% графически, есть все основания сомневаться в применимости метода в
% данной ситуации.

\subsection{Метод хи-квадрат построения прямой}
Пусть справедливы те же предположения, что и для метода наименьших квадратов,
но погрешности $\sigma_{i}$ экспериментальных точек различны. Метод
минимума хи-квадрат сводится к минимизации суммы квадратов отклонений,
где каждое слагаемое взято с \emph{весом} $w_i = 1/\sigma_i^2$:
 \[
 \chi^2(k,b)=\sum\limits _{i=1}^{n}w_{i} \left(y_i-(kx_i+b)\right)^{2}\to\mathrm{min}.
\]
Этот метод также называют \emph{взвешенным} методом наименьших квадратов.

Определим \emph{взвешенное среднее} от
некоторого набора значений $\left\{x_{i}\right\}$ как
\[
\left\langle x\right\rangle ^{\prime}=\frac{1}{W}\sum_{i}w_{i}x_{i},
\]
где $W=\sum\limits_{i}w_{i}$~--- нормировочная константа.
% Далее в этом разделе штрих будем для краткости опускать.

Повторяя процедуру, использованную при выводе \eqref{eq:MNK}, нетрудно
получить (получите) совершенно аналогичные формулы для искомых коэффициентов:
\begin{equation}
k=\frac{\average{xy}' - \average{x}'\average{y}'}%
    {\average{x^2}' - \average{x}'^2},\qquad
    b=\average{y}' -k \average{x}',\label{eq:MMP}
\end{equation}
с тем отличием от \eqref{eq:MNK}, что под угловыми скобками
$\left\langle \ldots\right\rangle'$
теперь надо понимать усреднение с весами $w_{i}=1/\sigma_{i}^{2}$.

Записанные формулы позволяют вычислить коэффициенты прямой,
\emph{если} известны погрешности $\sigma_{y_{i}}$. Значения $\sigma_{y_{i}}$
могут быть получены либо из некоторой теории, либо измерены непосредственно
(многократным повторением измерений при каждом $x_{i}$), либо оценены из
каких-то дополнительных соображений (например, как инструментальная погрешность).



\section{Свойства точечных оценок} \label{sec:point}
Если измеряется одна физическая величина $x$, то можно поставить задачу
по конечному набору данных $\mathbf{x}=\{x_i\}$ ($i=1\ldots n$) оценить параметры
случайного распределения, которому подчиняется $x$. В частности,
найти среднее значение (математическое ожидание)~$\limaverage{x}$ и
дисперсию~$\sigma^2$.

Если результатом оценки параметра является просто число~---
без указания интервала, в котором может лежать истинное значение,~---
такую оценку называют \emph{точечной}.
Пример точечных оценок дают формулы для выборочного среднего
\eqref{eq:average}:
\begin{equation}
\limaverage{x} \approx \average{x} = \frac{1}{n}\sum_i x_i\qquad
\label{eq:estimate_x_avg}
\end{equation}
и выборочной дисперсии \eqref{eq:sigma}:
\begin{equation}
\sigma^2 \approx s^2_n = \frac{1}{n}\sum_i (x_i - \average{x})^2.
\label{eq:estimate_x_sigma}
\end{equation}

% Точечная оценка сама по себе не имеет смысла с точки зрения физики, поскольку не
% позволяет определить погрешность результата. Поэтому для любого метода
% оценивания, применяемом в физике необходимо определить процедуру определения
% погрешности или интервала параметров с фиксированным вероятностным содержанием.

Оценка параметров должна давать правильное значение
хотя бы в пределе большого числа измерений.
Если при $n\to \infty$ оценка стремится к истинному значению параметра,
\[
\lim_{n\to \infty} \hat{\theta}(\mathbf{x}) \to \limaverage{\theta},
\]
то её называют \emph{состоятельной}.
Можно показать (см. []), что если у распределения,
которому подчиняется случайная величина,
существуют конечные средние и дисперсия, то оценки
\eqref{eq:estimate_x_avg}, \eqref{eq:estimate_x_sigma} являются состоятельными.

\paragraph{Несмещенные оценки.}
Рассмотрим случай малого числа измерений ($n\gtrsim 1$).
Тогда даже если оценка состоятельна, она может давать довольно большую ошибку.
При фиксированном $n$ функцию оценки $\hat{\theta}(x_1,\ldots,x_n)$
можно рассматривать как случайную величину с некоторым распределением,
отличающимся от распределения измеряемой величины.
Естественно потребовать, чтобы среднее (математическое ожидание) этого
распределения совпадало с истинным значением искомого параметра:
\[
\limaverage{\strut\hat{\theta}(\mathbf{x})} = \limaverage{\theta}.
\]
В таком случае оценку называют \emph{несмещённой}.

Нетрудно показать, что выборочное среднее \eqref{eq:estimate_x_avg}
является несмещённой оценкой. А вот оценка $s_n^2$ из
\eqref{eq:estimate_x_sigma} таким свойством не обладает. Математическое
ожидания для величины $s_n^2$ при фиксированном $n$ оказывается равно
$\limaverage{s_n^2} = \frac{n-1}{n} \sigma^2$ (предлагаем в качестве упражнения
проверить данное утверждение самостоятельно). Именно поэтому при малых $n$
для оценки дисперсии рекомендуется использовать формулу
\eqref{eq:sigma_straight}:
\[
\sigma^2 \approx s^2_{n-1} = \frac{1}{n-1}\sum_i (x_i - \average{x})^2.
\]

% \example{Рассмотрим формулу \eqref{eq:estimate_x_sigma} для среднеквадратичного
%     отклонения при $n=2$. Выборочное среднее равно
%     $\average{x}=\frac{x_{1}+x_{2}}{2}$, выборочная дисперсия:
%     \[
%     s_{x}^{2}=\frac{1}{2}\left[\left(x_{1}-\frac{x_{1}+x_{2}}{2}\right)^{2}+\left(x_{2}-\frac{x_{1}+x_{2}}{2}\right)^{2}\right]=\frac{1}{4}x_{1}^{2}-\frac{1}{2}x_{1}x_{2}+\frac{1}{4}x_{2}^{2}.
%     \]
%     Рассмотрим выражение для $s_x^2$ как случайную функцию двух случайных переменных
%     $x_1$ и $x_2$, и найдём её математическое ожидание
%     (то есть усредним $s_x^2$ по большому числу опытов, в каждом из которых
%     проведено по по $n=2$ измерений).
%     Учитывая, что $x_1$ и $x_2$~--- независимы
%     ($\limaverage{x_1x_2}=\limaverage{x}_1\cdot \limaverage{x}_2$), получим
%     \[
%     \limaverage{s_{x}^{2}}=\frac12 \limaverage{x^2}~--- \frac12 \limaverage{x}^2.
%     \]
%     Сравнивая с известной формулой
%     $\sigma^2 = \limaverage{x^2}~--- \limaverage{x}^2$,
%     видим, что среднее значение оценки отличается от истинного в 2 раза.
% }


\paragraph{Эффективность оценки.}
Для сравнения разных методов оценки очень важным свойством является
их \emph{эффективность}. На качественном уровне эффективность~--- величина,
обратная разбросу значений $\hat{\theta}(\mathbf{x})$ при применении к разным
наборам данных $\mathbf{x}$. Как обсуждалось выше, оценка $\hat{\theta}(\mathbf{x})$
есть случайная величина, подчиняющаяся некоторому, в общем случае неизвестному,
распределению. Среднее $\overline{\hat{\theta}(\mathbf{x})}$ по этому распределению
определяет смещение оценки.
А его дисперсия $\sigma^2\left(\hat\theta\right)$~--- как раз мера ошибки
в определении параметра. Выбирая между различными методами, мы, естественно,
хотим, чтобы ошибка была минимальной. Разные статистические методы обладают
разной эффективностью и в общем случае при конечном $n$ величина
$\sigma^2\left(\hat\theta\right)$ никогда не будет равна нулю.

Теорема, устанавливающая максимальное значение эффективности оценки,
рассмотрена в п.~\ref{sec:rao}.
% Разумеется, встает
% вопрос о том, можно ли построить оценку с максимальной возможной
% эффективностью.



