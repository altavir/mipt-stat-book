\chapter{Оценка параметров зависимостей}
\disclaimer{
    В этой главе представлена только очень краткая выжимка из теории оценок. Для
должного понимания этой главы математической статистики следует обратить
внимание на дополнительные материалы
% \todo{ссылка}.
}

\section{Оценка параметров}

Процедуру определения физических параметров по измерениями называют оцениванием,
а результат этой процедуры - оценкой. Такая терминология связана с тем, что
никакое физическое значение не может быть измерено с абсолютной точностью.
Всегда есть некоторая погрешность измерения. В простых случаях, оценки могут
быть получены простым усреднением результатов измерений или значений каких-то
функций этих результатов. Но в большинстве случаев измеряется не одна величина,
а зависимость одной величины от другой. В этом случае требуется построить оценку
параметра этой зависимости.

\example{
    Пусть есть резистор, подключенный к источнику постоянного напряжения
(напряжение можно менять), амперметр и вольтметр. Нужно определить сопротивление
$R$ этого резистора. Для того, чтобы получить значение можно провести ряд
измерений с одним или разными напряжениями $U$, посчитать в каждом случае
значение сопротивления как отношение напряжения к току: $R = U / I$, и усреднить
результаты. Такой метод можно использовать, но при этом результат базируется на
предположении о том, что зависимость напряжения от тока является строго
линейной, что, вообще говоря, соблюдается не всегда (к примеру может
существовать дополнительное паразитное сопротивление).

    Более корректным способом является измерение тока для различных значений
напряжения, построения линейной зависимости и оценки ее параметров. В этом
случае точность оценки остается такой же, но при этом возникает возможность
визуальной или аналитической проверки соответствия данных зависимости.
%     \todo[inline]{добавить картику?}
}

Для построения оценки нужны следующие компоненты:
\begin{itemize}
    \item \textbf{Данные}. Непосредственно измерения зависиомтси и погрешности
этих измерений. Экспериментальная погрешность является неотъемлемой частью
набора данных.
    \item \textbf{Модель}. Параметрическое описание зависимости. В примере выше
модель представляет собой прямую с одинм или двумя параметрами.
    \item \textbf{Оценщик}. Математическая процедура, которая по модели и данным
строит значения оценки.
\end{itemize}

Точечная оценка сама по себе не имеет смысла с точки зрения физики, поскольку не
позволяет определить погрешность результата. Поэтому для любого метода
оценивания, применяемом в физике необходимо определить процедуру определения
погрешности или интервала параметров с фиксированным вероятностным содержанием.

\section{Метод Пирсона (минимума \texorpdfstring{$\chi^{2}$}{chi2})}
\label{sec:chi2}

Наиболее распространенным способом оценки параметров является метом Пирсона,
который часто называют методом минимума $\chi^2$. Суть этого метода заключается
в том, что строится сумма вида:
\begin{equation}
    \chi^2 = \sum_i{\frac{(y_i - \mu(x_i,\theta))^2}{\sigma_i^2}},
\end{equation}
которую еще называют суммой $\chi^2$. Здесь $x_i$ и $y_i$ - координаты точек
зависимости, $\theta$ - исследуемый параметр или параметры, а $\mu$ - модель
(функция, описывающая зависимость). Погрешность измерений $\sigma_i$ как правило
указывают только для вертикальной шкалы ($y$). Оси стоит выбирать таким образом,
чтобы ошибка по вертикали была сильно больше, чем по горизонтали. Если ошибки
сопоставимы, то в качестве $\sigma_i$ можно брать среднеквадратичную ошибку по
обеим осям: $\sigma_i^2 = \sigma_{x,i}^2 + \sigma_{y,i}^2$.

Утверждается, что для суммы, определенной таким образом, значение параметра, при
котором $\chi^2$ будет минимальным будет давать корректную оценку этого
параметра (или параметров).

Оценку минимума $\chi^2$ можно получить как частный случай метода максимума
правдоподобия или из наивных соображений: очевидно, что чем ближе данные к
модельной кривой, тем меньше будет сумма.

\note{
    Процедура минимизации $\chi^2$ не имеет общего алгоритма. Для некоторых
частных случаев существуют аналитические решения, в частности, в разделе
\ref{sec:linear} рассмотрен случай линейной модели, но в общем случае следует
использовать соответствующие программы.
}

\subsection{Интервальная оценка}

Важным свойством метода Пирсона является встроенная возможность построения
интервальной оценки (нахождения погрешности). Для этого можно использовать тот
факт, что сама зависимость $\chi^2(\theta)$ в большинстве случае имеет вид
параболы, а величина $\exp{- \chi^2(\theta)/2}$ пропорциональна так называемой
функции правдоподобия, то есть вероятности получить заданный набор данных при
фиксированной модели и параметрах $\theta$. Легко видеть, что функция
правдоподобия в этом случае имеет вид нормального распределения и для
определения вероятностного содержания различных интервалов можно пользоваться
свойствами этого распределения. Для практической оценки достаточно найти такие
значения $\theta$, при которых значение $\chi^2$ будет отличаться от
минимального на 1. Эти значения будут соответствовать краям интервала с
вероятностным содержанием 68\% или $1-\sigma$ интервалу. Отклонение на 2 будет
соответствовать уже 95\% доверительному интервалу.

\note{
    Оценка методом минимума $\chi^2$ может быть использована в подавляющем
большинстве случаев, но все-таки не является универсальной. В случае с особо
точными измерениями рекомендуется обратиться к дополнительной литературе.
}

\subsection{Проверка качества фита}

Дополнительное полезоное свойство суммы $\chi^2$ заключается в том,
что она позволяет оценить, насколько хорошо данные описываются моделью.
В том случае, когда измерения распределены по нормальному закону и
независимы между собой, сумма $\chi^2$ оказывается распределена по
%
% \href{
% https://ru.wikipedia.org/wiki/\%D0\%A0\%D0\%B0\%D1\%81\%D0\%BF\%D1\%80\%D0\%B5\%
% D0\%B4\%D0\%B5\%D0\%BB\%D0\%B5\%D0\%BD\%D0\%B8\%D0\%B5_\%D1\%85\%D0\%B8-\%D0\%BA
% \%D0\%B2\%D0\%B0\%D0\%B4\%D1\%80\%D0\%B0\%D1\%82}
% {
одноименному распределению.
% }
При хорошем соответствии модели и данных величина
$\chi^2 / n $, где $n$ --- так называемое количество степеней
свободы (количество точек минус количество параметров), должна в среднем
быть равна 1. Значения существенно большие (2 и выше) свидетельствуют о
плохом соответствии или заниженных погрешностях. Значения меньше 0.5 как
правило свидетельствуют о завышенных ошибках.

\section{Метод наименьших квадратов}

    В случае, если все ошибки $\sigma_i$ одинаковы, множитель
$\frac{1}{\sigma^2}$ можно вынести за скобку. Для нахождения минимума
постоянный множитель не важен, поэтому мы можем назвать оценкой
наименьших кавдратов такое значение параметра $\hat\theta$, при
котором миниальна сумма квадратов:
\begin{equation}
  Q = \sum{(X_i - \mu_i(\theta))^2}.
\end{equation}
Эта оценка очевидно сохраняет все свойства оценки
минимума $\chi^2$, правда только в том случае, если все ошибки
действительно одинаковы.

Оценка наименьших квадратов удобна в том случае, когда не известны
ошибки отдельных измерений. Она является состоятельной и асимптотически
несмещенной, хотя ее эффективность оптимальна только для ограниченного
количества случаев. В некоторых случаях, ошибку измерения можно оценить
по разбросу данных, используя критерий Пирсона.

\paragraph{Интервальная оценка}

Оценка методом наименьших квадратов очевидно игнорирует информацию о
погрешностях измерений и не позволяет напрямую оценить погрешность
результата без дополнительных предположений. Для получения оценки
погрешностей надо сделать три предположения:

\begin{itemize}
    \item  Данные описываются предложенной моделью;
    \item  Отклонения данных от модельной кривой независимы между собой (носят
  статистический характер);
    \item  Статистические ошибки для всех точек равны между собой.
\end{itemize}

При этих предположениях, можно оценить $\sigma$ для каждой из точек
как средне квадратичное отклонение точек от наилучшей модели (той,
которая получена минимизацией суммы $Q$). После этого задача получения
погрешностей сводится к уже решенной для метода $\chi^2$.

\note{
По очевидным причинам оценка погрешностей, проведенная таким образом, не имеет
смысла для маленького количества измерений (меньше 8-10 точек). Все будет
работать и будет получен какой-то результат, но он будет довольно бессмысленным.
}


\note{
Одна из основных проблем, связанных с определением погрешностей методом
наименьших квадратов заключается в том, что он дает разумные погрешности даже в
том случае, когда данные вообще не соответствуют модели. По этой причине не
рекомендуется использовать его в тех случаях, когда погрешности измеренных
значений известны. Если других инструментов под рукой нет, то результаты работы
метода надо всегда проверять визуально по графику.
}



\section{Многопараметрические оценки}

    Однопараметрические оценки очень просты для понимания и реализации, но
довольно редко встречаются на практике. Даже при оценке параметров
линейно зависимости вида $y = k x + b$ уже существует два параметра:
$k$ - наклон прямой и $b$ - смещение. Все перечисленные выше
математические методы отлично работают и в многомерном случае, но
процесс поиска экстремума функции (максимума в случае метода максимума
правдоподобия и минимума в случае методов семейства наименьших
квадратов) и интерпретация результатов требуют использования специальных
программных пакетов.


\subsection{Доверительные области в многомерном случае}

    Принцип построения доверительной области в многомерном случае точно
такой же, как и для одномерных доверительных интервалов. Требуется найти
такую областью пространства параметров $\Omega$, для которой
вероятностное содержание для оценки параметра $\hat \theta$ (или
самого параметра $\theta$ в засимости от того, какой философии вы
придерживаетесь) будет равно некоторой наперед заданной величине
$\alpha$:
\begin{equation}
P(\theta \in \Omega) = \int_\Omega{L(X | \theta)}d\Omega = \alpha.
\end{equation}

Реализация на практике этого определения сталкивается с тремя
проблемами:

\begin{enumerate}
\item Взятие многомерного интеграла от произвольной функции - не тривиальная
  задача. Даже в случае двух параметров, уже требуется некоторый уровень
  владения вычислительной математикой и компьютерными методами. В случае
  большего числа параметров, как правило надо использовать специально
  разработанные для этого пакеты.
\item Определить центральный интервал для гипер-области гораздо сложнее, чем
  сделать это для одномерного отрезка. Единых правил для выбора такой
  области не существует.
\item Даже если удалось получить доверительную область, описать такой объект
  в общем случае не так просто, так что представление результатов
  составляет определенную сложность.
\end{enumerate}

Для решения этих проблем, пользуются следующим приемом: согласно
центральной предельной теореме, усреднение большого количества одинаково
распределенных величин дает нормально распределенную величину. Это же
верно и в многомерном случае. В большинстве случаев, мы ожидаем, что
функция правдоподобия будет похожа на многомерное нормальное
распределение:
\begin{equation}
    L(\theta) = \frac{1}{(2 \pi)^{n/2}\left|\Sigma\right|^{1/2}} e^{-\frac{1}{2}
(x - \mu)^T \Sigma^{-1} (x - \mu)},
\end{equation}
где n - размерность вектора параметров, $\mu$ - вектор
наиболее вероятных значений, а $\Sigma$ -
%
% \href{
% https://ru.wikipedia.org/wiki/\%D0\%9A\%D0\%BE\%D0\%B2\%D0\%B0\%D1\%80\%D0\%B8\%
% D0\%B0\%D1\%86\%D0\%B8\%D0\%BE\%D0\%BD\%D0\%BD\%D0\%B0\%D1\%8F_\%D0\%BC\%D0\%B0\
%D1\%82\%D1\%80\%D0\%B8\%D1\%86\%D0\%B0}
% {
ковариационная матрица
% }
распределения.

Для многомерного нормального распределения, линии постоянного уровня (то
есть поверхности, на которых значение плотности вероятности одинаковые)
имеют вид гипер-эллипса, определяемого уравнением
$(x - \mu)^T \Sigma^{-1} (x - \mu) = const$. Для любого вероятностного
содержания $\alpha$ можно подобрать эллипс, который будет
удовлетворять условию на вероятностное содержание. Интерес правда
редставляет не эллипс (в случае размерности больше двух, его просто
невозможно отобразить), а ковариацонная матрица. Диагональные элементы
этой матрицы являются дисперсиями соответствующих параметров (с учетом
всех корреляций параметров).


\section{Аналитическая оценка для линейной модели}
\label{sec:linear}

В случае, когда модель имеет линейный вид $y=kx+b$, оценка методом минимума
$\chi^2$ или наименьших квадратов может быть получена аналитически:
\[
\left\{ \left(x_{i},y_{i}\right),i=1\ldots n\right\} .
\]

Расстояние от экспериментальной точки от искомой прямой, измеренное
по вертикали, равно
\[
\Delta y_{i}=y_{i}-\left(kx_{i}+b\right).
\]
Найдём такие коэффициенты $k$ и $b$, чтобы сумма квадратов таких
расстояний для всех точек была минимальной:
\begin{equation}
S\!\left(k,b\right)=\sum\limits _{i=1}^{n}\Delta
y_{i}^{2}\to\mathrm{min}.\label{eq:mnk_S}
\end{equation}
Данный метод построения наилучшей прямой называют \emph{методом наименьших
квадратов} (МНК).

Рассмотрим сперва более простой частный случай. Пусть заведомо известно,
что искомая прямая проходит через ноль, то есть $b=0$ и $y=kx$.
Необходимое условие минимума функции $S\left(k\right)$, как известно,
есть равенство нулю её производной. Дифференцируя сумму (\ref{eq:mnk_S})
по $k$, считая все величины $\left\{ x_{i},\,y_{i}\right\} $ константами,
найдём
\[
\frac{dS}{dk}=-\sum\limits _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}\right)=0.
\]
Решая относительно $k$, находим
\[
k=\frac{\sum\limits _{i=1}^{n}x_{i}y_{i}}{\sum\limits _{i=1}^{n}x_{i}^{2}}.
\]
Поделив числитель и знаменатель на $n$, этот результат можно записать
более компактно:
\begin{equation}
\boxed{k=\frac{\left\langle xy\right\rangle }{\left\langle x^{2}\right\rangle
}}.\label{eq:MNK0}
\end{equation}
Угловые скобки означают усреднение по всем экспериментальным точкам:
\[
\left\langle \ldots\right\rangle \equiv\frac{1}{n}\sum\limits
_{i=1}^{n}\left(\ldots\right)_{i}
\]

В общем случае при $b\ne0$ функция $S\left(k,b\right)$ должна иметь
минимум как по $k$, так и по $b$. Поэтому имеем систему из двух
уравнений:
\begin{align*}
\frac{\partial S}{\partial k} & =-\sum\limits
_{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}-b\right)=0,\\
\frac{\partial S}{\partial b} & =-\sum\limits
_{i=1}^{n}2\left(y_{i}-kx_{i}-b\right)=0.
\end{align*}
Решая систему, можно получить
\begin{equation}
\boxed{k=\frac{\left\langle xy\right\rangle -\left\langle x\right\rangle
\left\langle y\right\rangle }{\left\langle x^{2}\right\rangle -\left\langle
x\right\rangle ^{2}},\qquad b=\left\langle y\right\rangle -k\left\langle
x\right\rangle }.\label{eq:MNK}
\end{equation}
Эти соотношения и есть решение задачи о построении наилучшей прямой
методом наименьших квадратов.

\note{
    Совсем кратко формулу (\ref{eq:MNK}) можно записать, если ввести обозначение
    \begin{equation}
        D_{xy}\equiv\left\langle xy\right\rangle -\left\langle x\right\rangle
\left\langle y\right\rangle =\left\langle \left(x-\left\langle x\right\rangle
\right)\cdot\left(y-\left\langle y\right\rangle \right)\right\rangle
\label{eq:cov}
    \end{equation}
    (в математической статистике $D_{xy}$ называют \emph{ковариацией};
    при $x\equiv y$ имеем дисперсию $D_{xx}=\left\langle \left(x-\left\langle
x\right\rangle \right)^{2}\right\rangle $).
    Тогда
    \begin{equation}
        k=\frac{D_{xy}}{D_{xx}},\qquad b=\left\langle y\right\rangle
-k\left\langle x\right\rangle .\label{eq:MNK_short}
    \end{equation}
}

\subsection{Погрешность МНК в линейной модели}

Найдём погрешности $\sigma_{k}$ и $\sigma_{b}$ коэффициентов, вычисленных
по формуле (\ref{eq:MNK}) (или (\ref{eq:MNK0})).

Сделаем следующие предположения: погрешность измерений величины $x$
пренебрежимо мала: $\sigma_{x}\approx0$, а погрешность по $y$ одинакова
для всех экспериментальных точек $\sigma_{y}=\mathrm{const}$ и имеет
случайный характер (систематическая погрешность отсутствует).

Пользуясь в этих предположениях формулами для погрешностей косвенных
измерений (см. раздел (\ref{sec:kosv})) можно получить следующие
соотношения (выкладки здесь весьма громоздки, подробности можно найти
в п. (\ref{subsec:MMP})):
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^{2}\right)},\label{
eq:MNK_sigma}
\end{equation}
\begin{equation}
\sigma_{b}=\sigma_{k}\sqrt{\left\langle x^{2}\right\rangle
},\label{eq:MNK_sigma_b}
\end{equation}
где использованы введённые выше сокращённые обозначения (\ref{eq:cov}).
Коэффициент $n-2$ отражает число независимых <<степеней
свободы>>: $n$ экспериментальных точек за вычетом двух
условий связи (\ref{eq:MNK}).

В частном случае $y=kx$ имеем
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-1}\left(\frac{\left\langle y^{2}\right\rangle
}{\left\langle x^{2}\right\rangle }-k^{2}\right)}.\label{eq:MNK_sigma0}
\end{equation}


\paragraph{Условия применимости МНК.}

Формулы (\ref{eq:MNK}) (или (\ref{eq:MNK0})) позволяют провести
прямую по \emph{любому} набору экспериментальных данных, а формулы
(\ref{eq:MNK_sigma}) (или (\ref{eq:MNK_sigma0})) --- вычислить
соответствующую среднеквадратичную ошибку для её коэффициентов. Однако
далеко не всегда результат будет иметь физический смысл. Перечислим
ограничения применимости данного метода.

В первую очередь метод наименьших квадратов --- статистический,
и поэтому он предполагает использование достаточно большого количества
экспериментальных точек (желательно $n>10$).

Поскольку метод предполагает наличие погрешностей только по $y$,
оси следует выбирать так, чтобы погрешность $\sigma_{x}$ откладываемой
по оси абсцисс величины была минимальна.

Кроме того, метод предполагает, что все погрешности в опыте ---
случайны. Соответственно, формулы (\ref{eq:MNK_sigma}) и (\ref{eq:MNK_sigma0})
применимы \emph{только для оценки случайной составляющей} ошибки $k$
и $b$. Если в опыте предполагаются достаточно большие систематические
ошибки, они должны быть оценены \emph{отдельно}. Отметим, что для
оценки систематических ошибок не существует строгих математических
методов, поэтому в таком случае проще и разумнее всего воспользоваться
описанным выше графическим методом.

Наконец, стоит предостеречь от использования МНК <<вслепую>>,
без построения графика. Этот метод неспособен выявить такие <<аномалии>>,
как отклонения от линейной зависимости, немонотонность, случайные
всплески и т.п. Все эти случаи могут быть легко обнаружены при построении
графика и требуют особого рассмотрения.

Резюмируя, можно сформулировать универсальную практическую рекомендацию:
если результаты какого-либо математического метода обработки данных
существенно расходятся с тем, что можно получить <<вручную>>
графически, есть все основания сомневаться в применимости метода в
данной ситуации.
