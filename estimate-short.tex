\chapter{Оценка параметров}
\label{ch:estimate}

% Процедуру определения физических параметров по измерениями называют оцениванием,
% а результат этой процедуры~--- оценкой. Такая терминология связана с тем, что
% никакое физическое значение не может быть измерено с абсолютной точностью.
% Всегда есть некоторая погрешность измерения. В простых случаях, оценки могут
% быть получены простым усреднением результатов измерений или значений каких-то
% функций этих результатов. Но в большинстве случаев измеряется не одна величина,
% а зависимость одной величины от другой. В этом случае требуется построить оценку
% параметра этой зависимости.

Цель любого физического эксперимента~--- проверить, выполняется ли некоторая
теоретическая закономерность (\emph{модель}), а также получить или уточнить
её параметры. Поскольку набор экспериментальных данных неизбежно ограничен,
а каждое отдельное измерение имеет погрешность, можно говорить лишь
об \emph{оценке} этих параметров. Как правило, измеряется не одна
величина, а некоторая функциональная зависимость величин друг от друга.
В таком случае возникает необходимость построить оценку параметров этой зависимости.

\example{Для измерения сопротивления некоторого резистора необходимо
    получить зависимость напряжения от тока $U(I)$.
Простейшая теоретическая модель для резистора~--- закон Ома $U=RI$,
где сопротивление $R$~--- единственный параметр модели. 
Можно также использовать модель с двумя параметрами $\{R,\,U_0\}$: $U=RI+U_0$,
которая позволяет корректно учесть часто возникающую в подобных измерениях
систематическую ошибку~--- смещение нуля напряжения или тока.
}

% \example{
%     Пусть есть резистор, подключенный к источнику постоянного напряжения
% (напряжение можно менять), амперметр и вольтметр. Нужно определить сопротивление
% $R$ этого резистора. Для того, чтобы получить значение можно провести ряд
% измерений с одним или разными напряжениями $U$, посчитать в каждом случае
% значение сопротивления как отношение напряжения к току: $R = U / I$, и усреднить
% результаты. Такой метод можно использовать, но при этом результат базируется на
% предположении о том, что зависимость напряжения от тока является строго
% линейной, что, вообще говоря, соблюдается не всегда (к примеру может
% существовать дополнительное паразитное сопротивление).
%
%     Более корректным способом является измерение тока для различных значений
% напряжения, построения линейной зависимости и оценки ее параметров. В этом
% случае точность оценки остается такой же, но при этом возникает возможность
% визуальной или аналитической проверки соответствия данных зависимости.
% %     \todo[inline]{добавить картику?}
% }

В~общем случае для построения оценки нужны следующие компоненты.
1)~\emph{данные}~--- результаты измерений $\{x_i, y_i\}$
и их погрешности $\{\sigma_i\}$
    (экспериментальная погрешность является неотъемлемой
    частью набора данных!).
2)~\emph{модель} $y=f(x\,|\,{\theta_1,\theta_2,\ldots })$~---
параметрическое описание исследуемой зависимости.
Здесь $\theta$~--- набор параметров модели, например,
коэффициенты $\{k,\,b\}$ прямой $f(x)=kx+b$.
3)~процедура построения оценки параметров по измеренным данным (\textquote{оценщик}): 
$\theta \approx \hat{\theta}(\{x_i,\,y_i,\,\sigma_i\})$.
%%Лишняя терминология?


Рассмотрим самые распространенные способы построения оценки.

\section{Метод минимума хи-квадрат}
\label{sec:chi2}

Обозначим отклонения результатов некоторой серии измерений от теоретической
модели $y=f(x\,|\, \theta)$ как
\[
 \Delta y_i = y_i- f(x_i\,|\,\theta),\qquad i= 1\ldots n,
\]
где $\theta$~--- некоторый параметр (или набор параметров),
для которого требуется построить наилучшую оценку. Нормируем $\Delta y_i$
на стандартные отклонения $\sigma_i$ и построим сумму
\begin{equation}
    \chi^2 = \sum_i{\left(\frac{\Delta y_i}{\sigma_i}\right)^2},
    \label{eq:chi2}
\end{equation}
которую принято называть суммой \emph{хи-квадрат}.

Метод \emph{минимума хи-квадрат} (\emph{метод Пирсона}) заключается в подборе такого
$\theta$, при котором сумма квадратов отклонений от теоретической
модели, нормированных на ошибки измерений, достигает минимума:
\[
\chi^2(\theta) \to \mathrm{min}.
\]

\note{Подразумевается, что погрешность измерений $\sigma_i$ указана только для
вертикальной оси $y$. Поэтому, при использовании метода следует выбирать оcи
таким образом, чтобы относительная ошибка по оси абсцисс была значительно меньше,
чем по оси ординат.}
% % а если у осей разные размерности???
% Если ошибки сопоставимы, то в качестве $\sigma_i$ можно брать среднеквадратичную ошибку по
% обеим осям: $\sigma_i^2 = \sigma_{x,i}^2 + \sigma_{y,i}^2$.}

Данный метод вполне соответствует нашему интуитивному представлению
о том, как теоретическая зависимость должна проходить через экспериментальные
точки. Ясно, что чем ближе данные к модельной кривой, тем
меньше будет сумма $\chi^2$. При этом, чем больше погрешность точки, тем
в большей степени дозволено результатам измерений отклоняться от модели.
Метода минимума $\chi^2$ является частным случаем
более общего \emph{метода максимума правдоподобия} (см. ниже),
реализующийся при \emph{нормальном} (\emph{гауссовом}) распределении ошибок.

%Можно показать (см. \cite{idie}), что оценка по методу хи-квадрат является состоятельной,
%несмещенной и, если данные распределены нормально,
%имеет максимальную эффективность (см. приложение \ref{sec:point}).

% \note{Оценка методом минимума $\chi^2$ может быть использована в подавляющем
% большинстве случаев, но все-таки не является универсальной. В случае с особо
% точными измерениями рекомендуется обратиться к дополнительной литературе.}

\note{Простые аналитические выражения для оценки методом хи-квадрат существуют
    (см. п. \ref{sec:MNK}, \ref{sec:linchi2}) только в случае линейной
    зависимости $f(x)=kx+b$ (нелинейную зависимость часто можно
    заменой переменных свести к линейной). В общем случае задача поиска
    минимума $\chi^2(\theta)$ решается численно, а соответствующая процедура
    реализована в большинстве специализированных программных пакетов
    по обработке данных.}


\section{Метод максимального правдоподобия.} 
Рассмотрим кратко один
из наиболее общих методов оценки параметров зависимостей~---
метод максимума правдоподобия.

Сделаем два ключевых предположения:
\begin{itemize}
 \item  зависимость между измеряемыми величинами действительно может
 быть описана функцией $y=f(x\,|\,\theta)$ при некотором $\theta$;
 \item все отклонения $\Delta y_i$ результатов измерений от теоретической модели
 являются \emph{независимыми} и имеют \emph{случайный} (не систематический!) характер.
\end{itemize}

Пусть $P(\Delta y_i)$~--- вероятность обнаружить отклонение $\Delta y_i$
при фиксированных $\{x_i\}$, погрешностях $\{\sigma_i\}$ и параметрах модели $\theta$.
Построим функцию, равную вероятности обнаружить
весь набор отклонений $\{\Delta y_1,\ldots,\Delta y_n\}$. Ввиду независимости
измерений она равна произведению вероятностей:
\begin{equation}\label{eq:L}
L =  \prod_{i=1}^n P(\Delta y_i).
\end{equation}
Функцию $L$ называют \emph{функцией правдоподобия}.

Метод максимума правдоподобия заключается в поиске такого $\theta$,
при котором наблюдаемое отклонение от модели будет иметь
\emph{наибольшую вероятность}, то есть
\[
L(\theta) \to \mathrm{max}.
\]
\note{Поскольку с суммой работать удобнее, чем с произведениями, чаще
    используют не саму функцию $L$, а её логарифм:
    \[
    \ln L = \sum_i \ln P(\Delta y_i).
    \]%
}

Пусть теперь ошибки измерений имеют \emph{нормальное} распределение.
Согласно \eqref{eq:normal}, вероятность обнаружить в $i$-м измерении
отклонение $\Delta y_i$ пропорциональна величине
\[
P(\Delta y_i) \propto e^{-\frac{\Delta y_i^2}{2\sigma_i^2}},
\]
где $\sigma_i$~--- стандартная ошибка измерения величины $y_i$. Тогда
логарифм функции правдоподобия \eqref{eq:L} будет равен (с точностью до константы)
\[
 \ln L = - \sum_i \frac{\Delta y_i^2}{2\sigma_i^2} = - \frac12 \chi^2.
\]
Таким образом, максимум правдоподобия действительно будет соответствовать
минимуму $\chi^2$.

\section{Метод наименьших квадратов (МНК)}

Рассмотрим случай, когда все погрешности измерений одинаковы,
$\sigma_i=\mathrm{const}$. Тогда множитель $1/\sigma^2$ в сумме 
хи-квадрат \eqref{eq:chi2} выносится за скобки, и оценка параметра сводится к нахождению минимума суммы квадратов отклонений:
\begin{equation}
S(\theta) = \sum_{i=1}^n \Delta y_i^2 \equiv \sum_{i=1}^n \Big(y_i - f(x_i\,|\,\theta)\Big)^2 \to \mathrm{min}.
\end{equation}

% Эта оценка очевидно сохраняет все свойства оценки
% минимума $\chi^2$, правда только в том случае, если все ошибки
% действительно одинаковы.
% Она является состоятельной и асимптотически
% несмещенной, хотя ее эффективность оптимальна только для ограниченного
% количества случаев. В некоторых случаях, ошибку измерения можно оценить
% по разбросу данных, используя критерий Пирсона.
%% это как?

Оценка по \emph{методу наименьших квадратов} (МНК) удобна в том случае,
когда не известны погрешности отдельных измерений. 
Для построения прямой $y=kx+b$ по методу МНК существуют простые аналитические 
выражения (см. п.~\ref{sec:MNK}).
Однако тот  факт, что
метод МНК игнорирует информацию о погрешностях, является и его основным
недостатком. В~частности, это не позволяет определить точность оценки
(например, погрешности коэффициентов прямой $\sigma_k$ и
$\sigma_b$) без привлечения дополнительных предположений
(см. п.~\ref{sec:MNKerror} и~\ref{sec:MNKdefect}).

% \paragraph{Интервальная оценка}

% Оценка методом наименьших квадратов очевидно игнорирует информацию о
% погрешностях измерений и не позволяет напрямую оценить погрешность
% результата без дополнительных предположений.
% Для получения оценки
% погрешностей надо сделать три предположения:
% %% предположения уже сделаны выше
% \begin{itemize}
%     \item  Данные описываются предложенной моделью;
%     \item  Отклонения данных от модельной кривой независимы между собой (носят
%   статистический характер);
%     \item  Статистические ошибки для всех точек равны между собой.
% \end{itemize}

% Оценить ошибку результатов оценки по методу наименьших квадратов
% (например, погрешности коэффициентов прямой $\sigma_k$ и $\sigma_b$
% можно следующим образом. В рамках сделанных предположениях
% (ошибки \emph{независимы} и \emph{одинаковы})
% величину $\sigma$ можно оценить для каждой из точек
% как средне квадратичное отклонение точек от наилучшей модели (той,
% которая получена минимизацией суммы $Q$). После этого задача получения
% погрешностей сводится к уже решенной для метода $\chi^2$.
%


\section{Проверка качества аппроксимации}

Значение суммы $\chi^2$ позволяет оценить, насколько хорошо данные описываются
предлагаемой моделью $y=f(x\,|\,\theta)$.

Предположим, что распределение ошибок при измерениях \emph{нормальное}.
Тогда можно ожидать, что большая часть отклонений данных от модели будет
порядка одной среднеквадратичной ошибки: $\Delta y_{i}\sim\sigma_{i}$.
Следовательно, сумма хи-квадрат \eqref{eq:chi2} окажется по порядку
величины равна числу входящих в неё слагаемых: $\chi^{2}\sim n$.

\note{Точнее, если функция $f\!\left(x\,|\,\theta_1,\,\ldots,\, \theta_p\right)$
содержит~$p$ подгоночных параметров
(например, $p=2$ для линейной зависимости $f\!\left(x\right)=kx+b$),
то при заданных $\theta$ лишь $n-p$ слагаемых в сумме хи-квадрат будут независимы.
Иными словами, когда параметры $\theta$ определены
из условия минимума хи-квадрат, сумму $\chi^{2}$ можно рассматривать как функцию
$n-p$ переменных. Величину $n-p$ называют \emph{числом степеней свободы} задачи.}

В теории вероятностей доказывается \cite{hudson, idie},
что ожидаемое среднее значение (математическое ожидание) суммы $\chi^{2}$
в точности равно числу степеней свободы:
\[
\limaverage{\chi^{2}}=n-p.
\]
Таким образом, при хорошем соответствии модели и данных,
величина $\chi^2 / (n-p) $ должна в среднем быть равна единице.
Значения существенно большие (2 и выше) свидетельствуют либо о
\emph{плохом соответствии теории и результатов измерений},
    либо о \emph{заниженных погрешностях}.
    Значения меньше 0,5 как правило свидетельствуют о \emph{завышенных погрешностях}.

\note{Чтобы дать строгий количественный критерий, с какой долей вероятности
    гипотезу $y=f\!\left(x\right)$ можно считать подтверждённой или опровергнутой,
    нужно знать вероятностный закон, которому подчиняется функция $\chi^{2}$.
    Если ошибки измерений распределены нормально, величина хи-квадрат подчинятся
    одноимённому распределению (с $n-p$ степенями свободы).
    В элементарных функциях \emph{распределение хи-квадрат} не выражается,
    но может быть легко найдено численно: функция встроена во все основные
    статистические пакеты, либо может быть  найдена по таблицам.}

\section{Оценка погрешности параметров}

Важным свойством метода хи-квадрат является \textquote{встроенная} возможность
нахождения погрешности вычисленных параметров $\sigma_{\theta}$.

Пусть функция $L(\theta)$ имеет максимум при $\theta = \hat{\theta}$, то есть 
$\hat{\theta}$ --- решение задачи о максимуме правдоподобия. Согласно центральной предельной теореме мы ожидаем, что функция правдоподобия будем близка к нормальному распределению: $L(\theta) \propto \exp\left(-\frac{(\theta - \hat{\theta})^2}{2 \sigma_\theta^2}\right) $, 
где $\sigma_\theta$ --- искомая погрешность параметра. Тогда в окрестности $\hat{\theta}$ функция $\chi^2(\theta) = -2 \ln(L(\theta))$ имеет вид параболы:
\[
\chi^2(\theta) = \frac{(\theta - \hat{\theta})^2}{\sigma_{\theta}^2} +\mathrm{const}.
\]
Легко убедиться, что:
\begin{equation*}
    \chi^2(\hat{\theta} \pm \sigma_\theta) - \chi^2(\hat{\theta}) = 1.
\end{equation*}
Иными словами, при отклонении параметра $\theta$ на одну ошибку $\sigma_{\theta}$ от значения
$\hat{\theta}$, 
минимизирующего $\chi^2$, функция $\chi^2(\theta)$ изменится на единицу. Таким образом для нахождения \emph{интервальной} оценки для искомого параметра достаточно графическим или численным образом решить уравнение
\begin{equation}
    \label{eq:deltachi2}
    \Delta \chi^2(\theta) = 1.
\end{equation}
Вероятностное содержание этого интервала будет равно 68\% (его еще называют 1--$\sigma$ интервалом). 
Отклонение $\chi^2$ на 2 будет соответствовать уже 95\% доверительному интервалу.

%\note{
%    Приведенное решение просто использовать только в случае одного параметра. Впрочем, все приведенные рассуждения верны и в много-параметрическом случае. Просто решением уравнения \ref{eq:deltachi2} будет не отрезок, а некоторая многомерная фигура (эллипс в двумерном случае и гипер-эллипс при больших размерностях пространства параметров). Вероятностное содержание области, ограниченной такой фигурой будет уже не равно 68\%, но может быть вычислено по соответствующим таблицам. Подробнее о многомерном случае см. Приложение.
%%    в разделе \ref{sec:multiparam}.
%}

%Для решения этих проблем, пользуются следующим приемом: согласно
%центральной предельной теореме, усреднение большого количества одинаково
%распределенных величин дает нормально распределенную величину. Это же
%верно и в многомерном случае. В большинстве случаев, мы ожидаем, что
%функция правдоподобия будет похожа на многомерное нормальное
%распределение:
%\begin{equation}
%    L(\theta) = \frac{1}{(2 \pi)^{n/2}\left|\Sigma\right|^{1/2}} e^{-\frac{1}{2}
%(x - \mu)^T \Sigma^{-1} (x - \mu)},
%\end{equation}
%где n - размерность вектора параметров, $\mu$ - вектор
%наиболее вероятных значений, а $\Sigma$ - ковариационная матрица распределения.

%Для многомерного нормального распределения, линии постоянного уровня (то
%есть поверхности, на которых значение плотности вероятности одинаковые)
%имеют вид гипер-эллипса, определяемого уравнением
%$(x - \mu)^T \Sigma^{-1} (x - \mu) = const$. Для любого вероятностного
%содержания $\alpha$ можно подобрать эллипс, который будет
%удовлетворять условию на вероятностное содержание. Интерес правда
%редставляет не эллипс (в случае размерности больше двух, его просто
%невозможно отобразить), а ковариацонная матрица. Диагональные элементы
%этой матрицы являются дисперсиями соответствующих параметров (с учетом
%всех корреляций параметров).

\section{Методы построения наилучшей прямой}
Применим перечисленные выше методы к задаче о построении наилучшей прямой
$y=kx+b$ по экспериментальным точкам $\{x_i,\,y_i\}$.
Линейность функции позволяет записать решение в относительно
простом аналитическом виде.

Обозначим расстояние от $i$-й экспериментальной точки до искомой прямой,
измеренное по вертикали, как
\[
\Delta y_{i}=y_{i}-\left(kx_{i}+b\right),
\]
и найдём такие параметры $\{k,b\}$, чтобы \textquote{совокупное} отклонение
результатов от линейной зависимости было в некотором смысле минимально.

\subsection{Метод наименьших квадратов}\label{sec:MNK}
\label{sec:linear}
Пусть сумма квадратов расстояний от точек до прямой минимальна:
\begin{equation}\label{eq:mnk_S}
S\!\left(k,b\right)=
\sum\limits _{i=1}^{n}(y_i-(kx_i+b))^{2}\to\mathrm{min}.
\end{equation}
Данный метод построения наилучшей прямой называют \emph{методом наименьших
квадратов} (МНК).

Рассмотрим сперва более простой частный случай, когда искомая прямая
заведомо проходит через \textquote{ноль}, то есть $b=0$ и $y=kx$.
Необходимое условие минимума функции $S\left(k\right)$, как известно,
есть равенство нулю её производной. Дифференцируя сумму (\ref{eq:mnk_S})
по $k$, считая все величины $\left\{ x_{i},\,y_{i}\right\} $ константами,
найдём
\[
\frac{dS}{dk}=-\sum\limits _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}\right)=0.
\]
Решая относительно $k$, находим
\[
k=\sum\limits_{i=1}^{n}x_{i}y_{i} \Big/
\sum\limits_{i=1}^{n}x_{i}^{2}.
\]
Поделив числитель и знаменатель на $n$, этот результат можно записать
более компактно:
\begin{equation}
k=\frac{\left\langle xy\right\rangle }{\left\langle x^{2}\right\rangle},
\label{eq:MNK0}
\end{equation}
где, напомним, угловые скобки обозначают выборочное среднее (по всем
экспериментальным точкам).
%Напомним, что угловые скобки означают усреднение по всем экспериментальным точкам:
%\[
%\left\langle \ldots\right\rangle \equiv\frac{1}{n}\sum\limits
%_{i=1}^{n}\left(\ldots\right)_{i}
%\]

В общем случае при $b\ne0$ функция $S\left(k,b\right)$ должна иметь
минимум как по $k$, так и по $b$. Поэтому имеем систему из двух
уравнений $\partial S/\partial k=0$, $\partial S/\partial b=0$,
% \begin{align*}
% \frac{\partial S}{\partial k} & =-\sum\limits
% _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}-b\right)=0,\\
% \frac{\partial S}{\partial b} & =-\sum\limits
% _{i=1}^{n}2\left(y_{i}-kx_{i}-b\right)=0.
% \end{align*}
решая которую, можно получить (получите самостоятельно):
\begin{equation}
    k=\frac{\average{xy}-\average{x}\average{y}}%
        {\average{x^{2}}-\average{x}^2},\qquad
        b=\average{y}-k\average{x}.\label{eq:MNK}
\end{equation}
Эти соотношения и есть решение задачи о построении наилучшей прямой
методом наименьших квадратов.

\note{Совсем кратко формулу (\ref{eq:MNK}) можно записать, если ввести обозначение
    \begin{equation}
        D_{xy}\equiv \average{\Delta x \cdot \Delta y} = \average{xy} - \average{x}\average{y}.
\label{eq:cov}
    \end{equation}
В математической статистике величину $D_{xy}$ называют \emph{ковариацией}.
При $y\equiv x$ имеем дисперсию
$D_{xx}=\average{\Delta x^2}$.
    Тогда
    \begin{equation}
        k=\frac{D_{xy}}{D_{xx}},\qquad b=\left\langle y\right\rangle
-k\left\langle x\right\rangle .\label{eq:MNK_short}
    \end{equation}
}

\subsection{Погрешность МНК в линейной модели}\label{sec:MNKerror}

Погрешности $\sigma_{k}$ и $\sigma_{b}$ коэффициентов, вычисленных
по формуле (\ref{eq:MNK}) (или (\ref{eq:MNK0})), можно оценить в
следующих предположениях.
Пусть погрешность измерений величины~$x$ пренебрежимо мала: $\sigma_{x}\approx0$,
а погрешности по~$y$ \emph{одинаковы} для всех экспериментальных точек
$\sigma_{y}=\mathrm{const}$, \emph{независимы} и имеют \emph{случайный} характер
(систематическая погрешность отсутствует).

Пользуясь в этих предположениях формулами для погрешностей косвенных
измерений (см. п.~\ref{sec:kosv}) можно получить следующие
соотношения:
%(выкладки здесь весьма громоздки, подробности можно найти в п. \ref{subsec:MMP} \todo{Можно?}):
\begin{equation}
    \sigma_{k}=
    \sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^{2}\right)},\quad
%    \label{eq:MNK_sigma_k}
%\end{equation}
%\begin{equation}
    \sigma_{b}=\sigma_{k}
    \sqrt{\left\langle x^{2}\right\rangle},
    \label{eq:MNK_sigma_b}
\end{equation}
где использованы введённые выше сокращённые обозначения \eqref{eq:cov}.
Коэффициент $n-2$ отражает число независимых <<степеней
свободы>>: $n$ экспериментальных точек за вычетом двух
условий связи \eqref{eq:MNK}.

В частном случае $y=kx$:
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-1}\left(\frac{\left\langle y^{2}\right\rangle
}{\left\langle x^{2}\right\rangle }-k^{2}\right)}.\label{eq:MNK_sigma0}
\end{equation}


\subsection{Метод хи-квадрат построения прямой}
\label{sec:linchi2}
Пусть справедливы те же предположения, что и для метода наименьших квадратов,
но погрешности $\sigma_{i}$ экспериментальных точек различны. Метод
минимума хи-квадрат сводится к минимизации суммы квадратов отклонений,
где каждое слагаемое взято с \emph{весом} $w_i = 1/\sigma_i^2$:
 \[
 \chi^2(k,b)=\sum\limits _{i=1}^{n}w_{i} \left(y_i-(kx_i+b)\right)^{2}\to\mathrm{min}.
\]
Этот метод также называют \emph{взвешенным} методом наименьших квадратов.

Определим \emph{взвешенное среднее} от
некоторого набора значений $\left\{x_{i}\right\}$ как
\[
\left\langle x\right\rangle ^{\prime}=\frac{1}{W}\sum_{i}w_{i}x_{i},
\]
где $W=\sum\limits_{i}w_{i}$~--- нормировочная константа.
% Далее в этом разделе штрих будем для краткости опускать.

Повторяя процедуру, использованную при выводе \eqref{eq:MNK}, нетрудно
получить (получите) совершенно аналогичные формулы для искомых коэффициентов:
\begin{equation}
k=\frac{\average{xy}' - \average{x}'\average{y}'}%
    {\average{x^2}' - \average{x}'^2},\qquad
    b=\average{y}' -k \average{x}',\label{eq:MMP}
\end{equation}
с тем отличием от \eqref{eq:MNK}, что под угловыми скобками
$\left\langle \ldots\right\rangle'$
теперь надо понимать усреднение с весами $w_{i}=1/\sigma_{i}^{2}$.

Записанные формулы позволяют вычислить коэффициенты прямой,
\emph{если} известны погрешности $\sigma_{y_{i}}$. Значения $\sigma_{y_{i}}$
могут быть получены либо из некоторой теории, либо измерены непосредственно
(многократным повторением измерений при каждом $x_{i}$), либо оценены из
каких-то дополнительных соображений (например, как инструментальная погрешность).


\subsection{Недостатки и условия применимости методов}\label{sec:MNKdefect}

Формулы (\ref{eq:MNK}) (или (\ref{eq:MNK0})) позволяют провести
прямую по \emph{любому} набору экспериментальных данных, 
а соотношения~\eqref{eq:MNK_sigma_b}~--- вычислить
соответствующую среднеквадратичную ошибку для её коэффициентов. Однако
далеко не всегда результат будет иметь физический смысл. Перечислим
ограничения применимости рассмотренных методов.

В первую очередь, все статистические методы, включая МНК,
предполагают для получения достоверных результатов использование достаточно большого количества экспериментальных точек (желательно $n>10$). 

Напомним, что всюду выше мы предполагали наличие погрешностей только по оси ординат, поэтому оси следует выбирать так, чтобы погрешность $\sigma_{x}$ величины, откладываемой по оси абсцисс, была минимальна.

Ещё одно предположение заключалось в том, что все погрешности опыта~---
\emph{случайны}. Поэтому, например, формулы (\ref{eq:MNK_sigma_b})--(\ref{eq:MNK_sigma0})
применимы \emph{только для оценки случайной составляющей} ошибки $k$
или $b$. Если в опыте предполагаются достаточно большие систематические
ошибки, они должны быть оценены \emph{отдельно}. Отметим, что для
оценки систематических ошибок не существует строгих математических
методов, поэтому в таком случае проще и разумнее всего воспользоваться
\emph{графическим} методом (см. п.~\ref{sec:graph}).

Одна из основных проблем именно метода МНК --- он не способен выявить ситуации, в которых разброс экспериментальных данных столь велик, что их нельзя считать соответствующими теоретической модели (метод всегда даст \textquote{разумный}
результат для коэффициентов и их погрешностей). Поэтому, если погрешности измерений известны, предпочтительно использовать метод минимума~$\chi^2$, лишённый данного недостатка.



% // не понятно!
%Одна из основных проблем, связанных с определением погрешностей методом
%наименьших квадратов заключается в том, что он дает разумные значения даже в том %случае, когда данные вообще не соответствуют модели.
%Если погрешности измерений известны, предпочтительно использовать
%метод минимума~$\chi^2$.
% Если других инструментов под рукой нет, то  результаты работы
% метода надо всегда проверять визуально по графику.

Наконец, стоит предостеречь от использования \emph{любых} аналитических
методов \textquote{вслепую}, без построения графиков. В частности, МНК не способен выявить такие \textquote{аномалии}, как отклонения от линейной зависимости, немонотонность, случайные всплески и т.\,п. Эти случаи требуют особого рассмотрения и могут быть легко обнаружены визуально по графику.



% Резюмируя, можно сформулировать универсальную практическую рекомендацию:
% если результаты какого-либо математического метода обработки данных
% существенно расходятся с тем, что можно получить \textquote{вручную}
% графически, есть все основания сомневаться в применимости метода в
% данной ситуации.
